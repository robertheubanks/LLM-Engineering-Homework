{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eo_QP1ITFfX2"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertheubanks/LLM-Engineering-Homework/blob/main/Eubanks_W1%20H2_Copy_of_Shakespeare_Generative_Model_from_Scratch_Assignment_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Pre-Training of GPT-Style Model\n",
        "\n",
        "In today's notebook, we'll be working through an example of how to do unsupervised pre-training of a GPT-style model.\n",
        "\n",
        "The base model we'll use is Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "All of the model code can be found in the [`model.py`](https://github.com/karpathy/nanoGPT/blob/master/model.py) file!\n",
        "\n",
        "> NOTE: We will not be leveraging the parallized training strategy in this notebook - you can find all the required code in the provided repository."
      ],
      "metadata": {
        "id": "UWiGVj6njoDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Selection\n",
        "\n",
        "For the notebook today, we'll be using a toy dataset called `tinyshakespeare`. Feel free to use your own corpus here, just make sure it's contained within a single `.txt` file.\n",
        "\n",
        "You could extend this example to use the [OpenWebText](https://skylion007.github.io/OpenWebTextCorpus/) dataset, which was used to pre-train GPT-2.\n",
        "\n",
        "> NOTE: Training LLMs can take a very long time - in order to get results similar to the [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) you will need 8xA100s and train for ~4-5 days using a pararellized strategy (DDP) on the OpenWebText Corpus.\n",
        "\n",
        "Let's start by grabbing our source repository for the day!"
      ],
      "metadata": {
        "id": "eHi04aEnkKEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMRsEQZy6tgc",
        "outputId": "0bcdb497-84c1-4247-85fe-d47c77d7899e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 649, done.\u001b[K\n",
            "remote: Total 649 (delta 0), reused 0 (delta 0), pack-reused 649\u001b[K\n",
            "Receiving objects: 100% (649/649), 936.45 KiB | 20.81 MiB/s, done.\n",
            "Resolving deltas: 100% (371/371), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll need to grab some dependencies.\n",
        "\n",
        "`cohere` and `openai` are recent dependencies of `tiktoken`, but we will not be leveraging them today."
      ],
      "metadata": {
        "id": "6l4CqoEDl7ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken requests cohere openai -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_gepPv1Qdj_",
        "outputId": "2398a11e-fe1c-40f1-9948-e43311726b18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.1/212.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First things first - let's download our dataset!\n",
        "\n",
        "We'll leverage the `requests` library to do this - and then we will split our resultant data into a `train` and `val` set."
      ],
      "metadata": {
        "id": "70hSjXmZmCt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "\n",
        "current_path = \"/data/shakespeare\"\n",
        "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not os.path.exists(current_path):\n",
        "    os.makedirs(current_path)\n",
        "\n",
        "# download the tiny shakespeare dataset\n",
        "input_file_path = os.path.join(os.path.dirname(current_path), 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]"
      ],
      "metadata": {
        "id": "T7qRWArUNiZ5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's get our `tokenizers` dependency so we can train a tokenizer on our data."
      ],
      "metadata": {
        "id": "wU9BG2CymU-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers -qU"
      ],
      "metadata": {
        "id": "gFnrwKpQPsYh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be training a \"byte-pair-encoding\" or \"BPE\" tokenizer. If you'd like to read more, you can find it [here](https://en.wikipedia.org/wiki/Byte_pair_encoding).\n",
        "\n",
        "Let's work through an example of what Byte-Pair Encoding (BPE) is doing, exactly, from this wonderful example provided by [Hugging Face](https://huggingface.co/docs/transformers/main/tokenizer_summary#byte-pair-encoding-bpe).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmWXE5ctma9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is BPE?\n",
        "\n",
        "First, we need to do a step called \"pre-tokenization\", which is - as it sounds - a tokenization step that occurs before we tokenize.\n",
        "\n",
        "The essential idea of BPE is that we need to understand common words and \"byte-pairs\" in them. So, in order to find \"common words\" we first need to find...words!\n",
        "\n",
        "Let's take the following text and break it apart into its word components.\n",
        "\n",
        "\n",
        "```\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "```\n",
        "\n",
        "A naive way to do this would just be by splitting on spaces...and that is indeed what technique was used in GPT-2."
      ],
      "metadata": {
        "id": "GLecDiHbogvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"\n",
        "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
        "\"\"\"\n",
        "\n",
        "naive_word_list = input_text.split()"
      ],
      "metadata": {
        "id": "m34NDAGCpiz6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can count our words and get their frequency."
      ],
      "metadata": {
        "id": "hR8k-2bopqjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "vocab_and_frequencies = defaultdict(int)\n",
        "\n",
        "for word in naive_word_list:\n",
        "  vocab_and_frequencies[\" \".join(list(word))] += 1\n",
        "\n",
        "sorted(vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_201bSQpvqD",
        "outputId": "e3030235-32a2-47be-9f7a-eb75cbdb629f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('t h e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find our \"base vocabulary\", which is going to be each symbol present in our original dataset."
      ],
      "metadata": {
        "id": "NckufSxxp-w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple, List, Set\n",
        "\n",
        "def find_vocabulary_size(current_vocab: Dict[str, int]) -> int:\n",
        "  vocab = set()\n",
        "\n",
        "  for word in current_vocab.keys():\n",
        "    for subword in word.split():\n",
        "      vocab.add(subword)\n",
        "\n",
        "  return len(vocab)"
      ],
      "metadata": {
        "id": "BNcjzjDvvKjp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pf3kCf-WvdBL",
        "outputId": "9500b867-8d60-4e3b-dd66-94e96d8f97ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, there are 36 symbols in our base vocabulary. Let's convert our data into a form where we can capture each symbol separately."
      ],
      "metadata": {
        "id": "VoMq7GhKqf7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can start constructing our pairs. We will look at all the pairs of symbols as they appear and take into consideration their frequency in our corpus."
      ],
      "metadata": {
        "id": "OGxrHYmftDTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pairs_and_frequencies(current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  pairs = {}\n",
        "\n",
        "  for word, frequency in current_vocab.items():\n",
        "    symbols = word.split()\n",
        "\n",
        "    for i in range(len(symbols) - 1):\n",
        "      pair = (symbols[i], symbols[i + 1])\n",
        "      current_frequency = pairs.get(pair, 0)\n",
        "      pairs[pair] = current_frequency + frequency\n",
        "\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "sTwvfTAErQN7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_and_frequencies = find_pairs_and_frequencies(vocab_and_frequencies)"
      ],
      "metadata": {
        "id": "FudOaKmYv9-y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIJfkk7wFYw",
        "outputId": "5df98d94-92b7-4c28-8df4-eae58e8befbd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('t', 'h'), 11),\n",
              " (('i', 'n'), 10),\n",
              " (('r', 'e'), 8),\n",
              " (('h', 'e'), 8),\n",
              " (('a', 't'), 7)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the frequent pairs - we can merge those pairs into a single token.\n",
        "\n",
        "Let's see how this process looks in code."
      ],
      "metadata": {
        "id": "OqORqdzwsZ6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def merge_vocab(most_common_pair: Tuple[str], current_vocab: Dict[str, int]) -> Dict[str, int]:\n",
        "  vocab_out = {}\n",
        "\n",
        "  pattern = re.escape(' '.join(most_common_pair))\n",
        "  replacement = ''.join(most_common_pair)\n",
        "\n",
        "  for word_in in current_vocab:\n",
        "      word_out = re.sub(pattern, replacement, word_in)\n",
        "      vocab_out[word_out] = current_vocab[word_in]\n",
        "\n",
        "  return vocab_out"
      ],
      "metadata": {
        "id": "L7ohHm2kshoY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab_and_frequencies = merge_vocab(\n",
        "    sorted(pairs_and_frequencies.items(), key = lambda x: x[1], reverse=True)[0][0],\n",
        "    vocab_and_frequencies\n",
        ")"
      ],
      "metadata": {
        "id": "Ab760KKuwzZ6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(new_vocab_and_frequencies.items(), key = lambda x: x[1], reverse=True)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0XtvLbpxbSx",
        "outputId": "3ace54b9-353d-4cca-d9ee-db142285b7fb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('th e', 8), ('a', 4), ('o f', 4), ('v o c a b u l a r y', 4), ('h a s', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After one merge, we can see that `t h` has been converted to `th`!\n",
        "\n",
        "Let's see how that impacted our vocabulary."
      ],
      "metadata": {
        "id": "9DPkBzj2u-me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_vocabulary_size(new_vocab_and_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO_xegCtxjQf",
        "outputId": "cf676fd5-cab8-40e0-db2d-65334ee51967"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our vocabulary has increased by 1 as we've added the `th` symbol to it!\n",
        "\n",
        "In essence, BPE will continue to do this process until your desired vocabulary size (a hyper-parameter) is met!"
      ],
      "metadata": {
        "id": "o3M13D60xzZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Our Tokenizer\n",
        "\n",
        "Now that we have some background on how BBPE works, lets move on to training our tokenizer for our model!\n",
        "\n",
        "Let's walk through the steps we'll take:\n",
        "\n",
        "1. Initialize our `Tokenizer` with a `BPE` model. Be sure to include the `unk_token`.\n",
        "\n",
        "  - [`Tokenizer`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizer)\n",
        "  - [`Models`](https://huggingface.co/docs/tokenizers/api/models#models)\n",
        "\n",
        "2. We'll include a normalizer, applied at the sequence level, and we'll use `NFD()` to do so. More reading on Unicode Normalization Forms [here](https://unicode.org/reports/tr15/#Normalization_Forms_Table).\n",
        "\n",
        "  - [`NFD()`](https://huggingface.co/docs/tokenizers/api/normalizers#tokenizers.normalizers.NFD)\n",
        "\n",
        "3. We'll also add our `ByteLevel()` pre-tokenizer, and our `ByteLevelDecoder()` decoder.\n",
        "\n",
        "  - [`ByteLevel()`](https://huggingface.co/docs/tokenizers/api/pre-tokenizers#tokenizers.pre_tokenizers.ByteLevel)\n",
        "  - [`ByteLevelDecoder()`](https://huggingface.co/docs/tokenizers/api/decoders#tokenizers.decoders.ByteLevel)"
      ],
      "metadata": {
        "id": "BePYCbHly02H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFD, Sequence\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.normalizer = Sequence([NFD()])\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "metadata": {
        "id": "OrztE09OPosB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll want to add some special tokens to our tokenizer to ensure in has access to common token patterns.\n",
        "\n",
        "Let's use the following:\n",
        "\n",
        "- `\"<s>\"`    : bos_token - beginning of sequence token\n",
        "- `\"</s>\"`   : eos_token - end of sequence token\n",
        "- `\"<pad>\"`  : padding_token - token used to pad sequences\n",
        "- `\"<unk>\"`  : unk_token - token used to represent unknown tokens.\n",
        "- `\"<mask>\"` : mask_token - token used to mask parts of our sequence\n",
        "\n",
        "We're also going to set a target vocabulary of 50,000 tokens."
      ],
      "metadata": {
        "id": "dDqkNNdM1KsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = BpeTrainer(\n",
        "    vocab_size=50000,\n",
        "    show_progress=True,\n",
        "    special_tokens=[\n",
        "      \"<s>\",\n",
        "      \"<pad>\",\n",
        "      \"</s>\",\n",
        "      \"<unk>\",\n",
        "      \"<mask>\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "x9iQVhN3P3RN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nothing left to do but point it at our data-source and let it train!\n",
        "\n",
        "We'll use the `.train()` method to accomplish this task.\n",
        "\n",
        "> NOTE: Pay attention to the desired inputs of the `.train()` method.\n",
        "\n",
        "- [`Tokenizer.train()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.train)"
      ],
      "metadata": {
        "id": "yQ8X9vZe2Fyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(files=[input_file_path], trainer=trainer)"
      ],
      "metadata": {
        "id": "LinLHotSP7gv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can save our tokenizer - and then load it as a `GPT2Tokenizer` through the Hugging Face Library!"
      ],
      "metadata": {
        "id": "V2JNYiqB2qKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/tokenizer'\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "tokenizer.model.save(save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk6QjDGHQy2K",
        "outputId": "1a6a0b17-444f-4286-8f01-a0ed6ec9483a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/tokenizer/vocab.json', '/content/tokenizer/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOOlbggdRFrN",
        "outputId": "443b0c2f-0bf5-4e44-ff74-76e4629ac7ca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_path, unk_token=\"[UNK]\")"
      ],
      "metadata": {
        "id": "us1vofdhQ45C"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how it tokenizes our inputs!"
      ],
      "metadata": {
        "id": "0-Bnq7lV2xWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence = \"Hark, my name be Romeo! I am but a beautiful summer's day!\""
      ],
      "metadata": {
        "id": "dnYnFa3fTRLf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = tokenizer.tokenize(input_sentence)\n",
        "tokenized_sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSHY5VufRbBj",
        "outputId": "c4d7e342-6705-4476-8487-1cc6278cf785"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hark',\n",
              " ',',\n",
              " 'Ä my',\n",
              " 'Ä name',\n",
              " 'Ä be',\n",
              " 'Ä Romeo',\n",
              " '!',\n",
              " 'Ä I',\n",
              " 'Ä am',\n",
              " 'Ä but',\n",
              " 'Ä a',\n",
              " 'Ä beautiful',\n",
              " 'Ä summer',\n",
              " \"'s\",\n",
              " 'Ä day',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "encoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZrWzQQlTU41",
        "outputId": "ac6b0abd-d34a-4d79-b9e2-f44a8edd0749"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12077, 9, 124, 637, 121, 826, 5, 87, 295, 219, 72, 9113, 2999, 141, 511, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = tokenizer.decode(encoded_tokens, clean_up_tokenization_spaces=False)\n",
        "decoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oS6lE-NLRnzk",
        "outputId": "f3ea7d16-891d-484b-e709-fbc1d8368d3d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hark, my name be Romeo! I am but a beautiful summer's day!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing Dataset\n",
        "\n",
        "Now that we have trained our tokenizer - let's create a dataset we can leverage with the `nanoGPT` library.\n",
        "\n",
        "We'll simply encode our training and validation data - and then save them in binary files for later!\n",
        "\n",
        "> NOTE: Pay attention to the format you want your dataset in. We want ids, which means we want to use the [`.encode()`](https://huggingface.co/docs/tokenizers/api/tokenizer#tokenizers.Tokenizer.encode) method of our tokenizer."
      ],
      "metadata": {
        "id": "ji3sF-rA21YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = tokenizer.encode(train_data)\n",
        "val_ids = tokenizer.encode(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "calHML6JPnCU",
        "outputId": "2e3ad2aa-6e21-4eed-8ba2-320873e66080"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 291,284 tokens\n",
            "val has 34,223 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export to bin files\n",
        "data_path = \"/data/shakespeare/\"\n",
        "\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(os.path.dirname(data_path), 'train.bin'))\n",
        "val_ids.tofile(os.path.join(os.path.dirname(data_path), 'val.bin'))"
      ],
      "metadata": {
        "id": "nKJ1KqiiPkRh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at our first 100 training tokens to see what format they are in!"
      ],
      "metadata": {
        "id": "DFbbvIi7xsgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9z7ia8AxqEn",
        "outputId": "73a6bdee-7e6b-4209-f557-f2f7cb7f9997"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  21,  388,  876,   13,   68, 6804,  373,  153, 2501,  622, 2092,\n",
              "          9,  496,  136,  433,   11,   68,   68,   16,   89,   13,   68,\n",
              "         34, 7882,    9,  433,   11,   68,   68,   21,  388,  876,   13,\n",
              "         68,   40,   73,  252,  227, 3778, 1304,  103,  781,  351,  103,\n",
              "       7504,   15,   68,   68,   16,   89,   13,   68,   33,   97, 5790,\n",
              "         11, 3778,   11,   68,   68,   21,  388,  876,   13,   68,   21,\n",
              "        388,    9,  104,  330, 3317, 1177,  145, 3563, 1766,  103,   80,\n",
              "       1006,   11,   68,   68,   16,   89,   13,   68, 7797,  330,  486,\n",
              "          9,  153,  330,  486,   11,   68,   68,   21,  388,  876,   13,\n",
              "         68], dtype=uint16)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ğŸ—ï¸Activity:\n",
        "\n",
        "Write Python code that will return the first 100 tokens as text.\n",
        "\n",
        "> HINT: An example of this code was used above!"
      ],
      "metadata": {
        "id": "aDbAZt4Lx12n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER:\n",
        "# For the tokenized training data as shown previously\n",
        "first_100_tokens = train_ids[:100]\n",
        "\n",
        "# Decoding the first 100 tokens back to text\n",
        "decoded_text = tokenizer.decode(first_100_tokens)\n",
        "\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "id": "6oaYCn0P9-LJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training The Model\n",
        "\n",
        "Now that we have our tokenized dataset, let's get to training our model!\n",
        "\n",
        "We have a lot of set-up to do before we click \"`.train()`\", so let's jump right into it!\n",
        "\n",
        "First, let's literally jump into the `nanoGPT` repository we cloned earlier."
      ],
      "metadata": {
        "id": "c0I3VrRC3XIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUU2jaalUdqm",
        "outputId": "b51d7de4-f227-46c5-e16b-a90fa14c196f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do some critical imports."
      ],
      "metadata": {
        "id": "13p1e8sa3k0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from the local repo\n",
        "from model import GPTConfig, GPT"
      ],
      "metadata": {
        "id": "weNR37BwUYNg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyper-Parameters\n",
        "\n",
        "We have a laundry list of hyper-parameters to set up - let's walk through them and what they mean."
      ],
      "metadata": {
        "id": "kY_vWZG-3uM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I/O\n",
        "\n",
        "- `out_dir` - simple enough, this is the output directory where our checkpoints are saved"
      ],
      "metadata": {
        "id": "OykCjVQK5EX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = 'out'"
      ],
      "metadata": {
        "id": "viM3qlWt5PVS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initialization\n",
        "\n",
        "Since we're training from scratch, we'll use `init_from = 'scratch'`."
      ],
      "metadata": {
        "id": "A5iwwrNL5H4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_from = 'scratch'"
      ],
      "metadata": {
        "id": "OK1z2m3C312T"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Eval and Logging\n",
        "\n",
        "- `eval_interval` - this is the number of steps between evaluation stages, we'll want to see this ~`250`. Our model will be incredibly prone to over-fitting, and this will let us monitor with relative frequency.\n",
        "- `log_interval` - this is how often our training progress will log. You can set this ~`10`. It's dealer's choice, really.\n",
        "- `eval_iters` - this is how *many* iterations we want to evaluate for.\n",
        "- `eval_only` - this would evaluate our model - but not train it. We'll leave this as `False` for now.\n",
        "- `always_save_checkpoint` - this will always save our most recent checkpoint, regardless of metrics. For this example, we'll set this to `True`."
      ],
      "metadata": {
        "id": "2YlolKOj4_dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 250\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "eval_only = False\n",
        "always_save_checkpoint = True"
      ],
      "metadata": {
        "id": "MbFN5Ltq4_mo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "We can set our dataset here - we'll use the one we created earlier!"
      ],
      "metadata": {
        "id": "a488zaF_4zQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = 'shakespeare'"
      ],
      "metadata": {
        "id": "_QC7vWXC40Hp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Typical Hyper-Parameters\n",
        "\n",
        "- `gradient_accumulation_steps` - we can use gradient accumulation to \"simulate\" larger batch sizes by combining multiple different optimization steps together, without needing the additional memory for large batch sizes. We don't need to worry so much about this for the toy problem - but this hyper-parameter can be configured for larger training runs. [Here](https://lightning.ai/blog/gradient-accumulation/) is some great reading on the topic.\n",
        "- `batch_size` - Typical batch_size - the larger the merrier (up to a point) we'll be using `16` to ensure we do not exceed the memory quota of our GPU.\n",
        "- `block_size` - this can be thought of as another term for the `context window` of our model. Since our model cannot take variable length inputs - we use this to set all inputs to our desired size. We'll use a value of `512` to ensure speedy training."
      ],
      "metadata": {
        "id": "XP9rBgGc426Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient_accumulation_steps = 1\n",
        "batch_size = 16\n",
        "block_size = 512"
      ],
      "metadata": {
        "id": "EM_ybLPP43Pd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Architecture\n",
        "\n",
        "- `n_layer` - this is the number of decoder layers we will use in our model. More would be considered better (up to a point) and the original GPT-2 paper uses `12`, but we will be using a truncated `6` for ease and speed of training.\n",
        "- `n_head` - this is the number of attention heads in each decoder layer!\n",
        "- `n_embd` - this is the embedding dimension of our model, this is analagous to our `model_d` from the previous notebook.\n",
        "- `dropout` - this sets our dropout value, since our model is small and going to be extremely prone to overfitting, consider setting this at a fairly aggresive level (`0.2` was used in the example training found in the notebook`).\n",
        "- `bias` - wether or not to use bias inside the LayerNorm/Linear layers.\n",
        "\n",
        "> NOTE: You need to ensure your `n_embd` is cleanly divided by your `n_head`. That is to say:\n",
        ">\n",
        "> `n_embd % n_head == 0`."
      ],
      "metadata": {
        "id": "UZ-8bDIY45GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layer = 6\n",
        "n_head = 6\n",
        "n_embd = 516\n",
        "dropout = 0.2\n",
        "bias = False"
      ],
      "metadata": {
        "id": "gMyyDBxB6k4H"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####â“Question:\n",
        "\n",
        "How many attention heads (total) will our final network have?\n",
        "\n"
      ],
      "metadata": {
        "id": "FiTyY5Cotwig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: The total number of attention heads in the final network is determined by two parameters: the number of decoder layers (n_layer) and the number of attention heads per layer (n_head). 6 x 6 = 36"
      ],
      "metadata": {
        "id": "i9tq5NlYAKB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optimizer Hyper-Parameters\n",
        "\n",
        "Basic Optimizer Hyper-Parameters:\n",
        "\n",
        "- `learning_rate` - it's our learning rate! We'll want to set this fairly high ~`1e-3` since we're training on such a small dataset.\n",
        "- `max_iters` - how many iterations do we train for. More iters means longer training times. Feel free to tinker with this value! `5000` is a great place to start.\n",
        "\n",
        "Learning Rate Decay Settings:\n",
        "\n",
        "- `decay_lr` - set decay flag\n",
        "- `weight_Decay` - how much to decay lr by\n",
        "- `lr_decay_iters` - should be set to ~max_iters.\n",
        "- `min_lr` - the minimum lr, should be ~ lr / 10\n",
        "\n",
        "Clipping and Warmup:\n",
        "\n",
        "- `grad_clip` - value to clip gradients to. useful for preventing vanishing gradients.\n",
        "- `warmup_iters` - how many iterations to warmup for. Warmup is useful to allow your training to slowly warmup. It will use a low lr for a number of steps to avoid any massive initial spikes. Since we're training a very small model - we can avoid using many wamrup steps.\n",
        "\n",
        "> NOTE: Many learnings taken from the [Chincilla paper](https://arxiv.org/pdf/2203.15556.pdf) for selecting default or appropriate values."
      ],
      "metadata": {
        "id": "3NWDTaAz7gwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adamw optimizer\n",
        "learning_rate = 1e-3\n",
        "max_iters = 5_000\n",
        "beta1 = 0.9\n",
        "beta2 = 0.99\n",
        "\n",
        "# lr decay settings\n",
        "decay_lr = True\n",
        "weight_decay = 1e-1\n",
        "lr_decay_iters = 5_000\n",
        "min_lr = 1e-4\n",
        "\n",
        "# clipping and warmup\n",
        "grad_clip = 1.0\n",
        "warmup_iters = 100"
      ],
      "metadata": {
        "id": "qe-669jwUptI"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These hyper-parameters are necessary to set given the task we're training and given the environment we're training in."
      ],
      "metadata": {
        "id": "ucldc4mz9yeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend = 'nccl'\n",
        "device = 'cuda'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "compile = True\n",
        "# -----------------------------------------------------------------------------\n",
        "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
        "config = {k: globals()[k] for k in config_keys}\n",
        "# -----------------------------------------------------------------------------\n",
        "master_process = True\n",
        "seed_offset = 0\n",
        "ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHiGlMOp8Nux",
        "outputId": "788b5a33-305f-4c3e-dc9d-276f295dc7bb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 8,192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Torch Settings\n",
        "\n",
        "We need to set a few `torch` settings, including the seed, to allow us to train correctly on our GPU.\n",
        "\n",
        "Not much is required for us to understand here - these are just necessary lines of code. Boilerplate."
      ],
      "metadata": {
        "id": "eKmdfbye-BNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "yh34QGD6VARU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader\n",
        "\n",
        "This block will:\n",
        "\n",
        "1. Set the data path\n",
        "2. Load the dataset we tokenized earlier from the `.bin` we saved\n",
        "3. Define a `get_batch` function that will return us a random section of our data as well as a the corresponding \"label\" for that data and move it to the GPU for easy use inside our training loop."
      ],
      "metadata": {
        "id": "gKeNwYaZ-Zoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join('/data', dataset)\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "tOjaPyJpVEgx"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at what an example of our batches would look like.\n",
        "\n",
        "To remind ourselves:\n",
        "\n",
        "- `train_data` - has ~2.9 million entries\n",
        "- `block_size` - is 512\n",
        "- `batch_size` - is 16"
      ],
      "metadata": {
        "id": "1Z7vMU34yRbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
        "x = torch.stack([torch.from_numpy((train_data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "y = torch.stack([torch.from_numpy((train_data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])"
      ],
      "metadata": {
        "id": "9_-Y5RZ-yX2a"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Our randomly selected indices were: {ix}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JDxXph4yh2g",
        "outputId": "70bc3866-426f-4d67-ba72-62b0e6d200ef"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our randomly selected indices were: tensor([ 99775, 155569, 263696,  32920,  52919, 231541, 153767, 229238, 136782,\n",
            "        263618,  39008,  14208,  39429, 189430, 194466,  76798])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The first 10 elements of `x` at the first randomly selected index is:\\n{x[0][:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAzHJH-kzK5E",
        "outputId": "157d2219-2dd8-4360-bfbc-801b4ae9a137"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 elements of `x` at the first randomly selected index is:\n",
            "tensor([   68,    16,    81,  2358, 19949,   116,   172,  1280,     9,    68])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The first 10 elements of `y` at the first randomly selected index is:\\n{y[0][:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbaF6v8ezkWn",
        "outputId": "e33cfbae-82b9-4207-a91f-8f55ec7ae1b2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 10 elements of `y` at the first randomly selected index is:\n",
            "tensor([   16,    81,  2358, 19949,   116,   172,  1280,     9,    68,    16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####â“Question:\n",
        "\n",
        "Both `x` and `y` are lists of tokens - as is expected - but what relationship to you notice between `x` and `y`?\n",
        "\n",
        "> HINT: What is our auto-regressive language model trying to predict?"
      ],
      "metadata": {
        "id": "zOwOEI0Pzntj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: the relationship between x and y in the dataloader is that y is the sequence of tokens that immediately follows the sequence in x. This setup is aligned with the training objective of an autoregressive language model, where the model learns to predict the next token in a sequence based on the previous tokens"
      ],
      "metadata": {
        "id": "V_EFzJrLCkU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the first component selects a random index from our training data (accounting for our block size)"
      ],
      "metadata": {
        "id": "N62oDfdWy0XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Initialization of Model\n",
        "\n",
        "Here we init our number of iterations as 0, and our best val loss as a very high number."
      ],
      "metadata": {
        "id": "EbDlW-68_atH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iter_num = 0\n",
        "best_val_loss = 1e9"
      ],
      "metadata": {
        "id": "6hsepdVBVzQU"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtain our vocab size from our trained tokenizer."
      ],
      "metadata": {
        "id": "A4Uj9qBI_vXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "meta_vocab_size = tokenizer.vocab_size\n",
        "meta_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m53DcCdFV0_a",
        "outputId": "1485529b-4af4-481c-f46b-0a1f64da0265"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20099"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create our model args dict."
      ],
      "metadata": {
        "id": "V7bcNelYARmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
        "                  bias=bias, vocab_size=None, dropout=dropout)"
      ],
      "metadata": {
        "id": "JfIWEbanV7ZS"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate our model with the provided `model_args`.\n",
        "\n",
        "These are derived from the hyper-parameters we set above."
      ],
      "metadata": {
        "id": "2WWcbkiCAUI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    if meta_vocab_size is None:\n",
        "        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xly4iA0V-vF",
        "outputId": "334eea1a-35eb-43da-d262-73fadeb3bfc7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 29.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we go! If you used the default values - you should have a model with 29.55M parameters!\n",
        "\n",
        "Let's set our block_size to the correct size as determined in our configuration steps."
      ],
      "metadata": {
        "id": "BpViOsxLAl6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size"
      ],
      "metadata": {
        "id": "TrEawNxdWRhm"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can look at our model in all its glory!"
      ],
      "metadata": {
        "id": "eRgguPLKAuZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaE3KSTnAtJs",
        "outputId": "67273e16-77c9-45f5-8973-37f1f69d19a2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(20099, 516)\n",
              "    (wpe): Embedding(512, 516)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=516, out_features=1548, bias=False)\n",
              "          (c_proj): Linear(in_features=516, out_features=516, bias=False)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=516, out_features=2064, bias=False)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=2064, out_features=516, bias=False)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=516, out_features=20099, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ğŸ—ï¸Activity:\n",
        "\n",
        "Label the following components of the transformer architecture with the module names of our newly device-cast model!\n",
        "\n",
        "The first layer is provided as an example.\n",
        "\n",
        "- Layer Normalization: `ln_f`\n",
        "- Linear Projection for Output: (lm_head)\n",
        "- Attention Mechanism: (attn)\n",
        "- Positional Encoding: (wpe)\n",
        "- Embeddings: (wte)\n",
        "- Feed Forward Network: (mlp)\n",
        "\n"
      ],
      "metadata": {
        "id": "oHVw3KF3uvIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our GradScaler - more information on this process [here](https://pytorch.org/docs/stable/amp.html#gradient-scaling)."
      ],
      "metadata": {
        "id": "LzoEY6gcBOSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
      ],
      "metadata": {
        "id": "BNUThRt4WT5H"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set up our optimizer below. Be sure to include the correct values. You can check the `model.py` file for more information on what is expected in the `configure_optimizers` method [here](https://github.com/karpathy/nanoGPT/blob/eba36e84649f3c6d840a93092cb779a260544d08/model.py#L263C85-L263C85)."
      ],
      "metadata": {
        "id": "6Zs5Hcf9BBUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay,\n",
        "    learning_rate,\n",
        "    (beta1, beta2),\n",
        "    device_type\n",
        ")\n",
        "\n",
        "checkpoint = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YesGeUnoWViL",
        "outputId": "3838e613-13c6-40cc-bf86-8ace343e23e0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 29,805,708 parameters\n",
            "num non-decayed parameter tensors: 13, with 6,708 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compile our model!\n",
        "\n",
        "If you're using the T4 or V100 instance of Colab - this will not provide a signficant speed-up, but if you're using Ampere architecture (A100) you should notice a significant difference between the compiled and uncompiled model.\n",
        "\n",
        "Read more about `torch.compile()` [here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)."
      ],
      "metadata": {
        "id": "ZF5YWJoKB4og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0FNU0T0WXdI",
        "outputId": "45b9a530-ccfd-4fe9-fd20-134efcb2754c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set up our loss estimation function here, which will help us estimate an arbitrarily accurate loss over either training or validation data by using many batches.\n",
        "\n",
        "You'll notice that we quickly convert the model into `.eval()` model and then back to `.train()` mode."
      ],
      "metadata": {
        "id": "p6lRcVsZCXRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "lUB5zVLVWbhM"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our LR Scheduler\n",
        "\n",
        "Beyond just slowly reducing our learning rate over time - we can use an LR Scheduler to allow us to move our learning according to a desired pattern.\n",
        "\n",
        "We will use a \"cosine with warmup\" schedule and our learning rate, thusly, will follow this pattern:\n",
        "\n",
        "![img](https://i.imgur.com/KoFEl0b.png)\n",
        "\n",
        "There are many different schedulers, and many different ways to handle learning rate, and you can read about just a few of them [here](https://d2l.ai/chapter_optimization/lr-scheduler.html)!"
      ],
      "metadata": {
        "id": "fLsOpaACDDkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "7-mNpWBSWdHh"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###â“Question:\n",
        "\n",
        "What advantages does a learning-rate scheduler have over a static learning rate?\n",
        "\n",
        "Feel free to consult and cite any resources you find!"
      ],
      "metadata": {
        "id": "UV0qN0fDwdOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: SOURCE = CHATGPT. I've read a lot about this topic in the past. To be quite frank, chatgpt does a nice job of bringing it all together and summarizing better than i could have.\n",
        "1.\tImproved Convergence: A learning-rate scheduler can help the model converge more quickly and effectively. Initially, a higher learning rate can be used to make large strides in the optimization landscape. As training progresses, reducing the learning rate helps to fine-tune the model parameters, allowing the model to converge to a more optimal set of weights and potentially a better local minimum.\n",
        "2.\tPreventing Overshooting: At the beginning of training, the model is far from the optimal weights, so a larger learning rate can accelerate learning. However, as it gets closer to the optimal point, a large learning rate can cause the model to overshoot the minimum. Gradually decreasing the learning rate helps in making smaller, more precise updates to the model weights, avoiding such overshooting.\n",
        "3.\tAdapting to the Training Phase: Different phases of training might require different learning rates. Early on, the model might benefit from a high learning rate to escape local minima, but as training progresses, a smaller learning rate is more suitable for detailed adjustments. A learning-rate scheduler can automate this adjustment process.\n",
        "4.\tAvoiding Plateaus: In many cases, training loss tends to plateau after certain epochs. Dynamically adjusting the learning rate can help to break out of these plateaus and continue reducing the loss.\n",
        "5.\tCustomization for Specific Tasks: Different tasks and datasets might benefit from different learning rate strategies. A learning-rate scheduler allows for the flexibility to tailor the learning rate to the specific needs of the task, which can be particularly useful in transfer learning or fine-tuning scenarios.\n",
        "6.\tWarm-up Period: Some schedulers include a warm-up period where the learning rate gradually increases from a low value to the initial learning rate. This warm-up can be beneficial for stabilizing the training process, especially in large models like Transformers, by ensuring that the model doesn't make overly large updates too early in training."
      ],
      "metadata": {
        "id": "bRfWm0HqHKlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to set some specific values in our env to allow training in Colab."
      ],
      "metadata": {
        "id": "cqFePCZmE1Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7nDL6s4YT6E",
        "outputId": "959ce9d5-53d6-4161-e511-387ec608fbab"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Training Loop\n",
        "\n",
        "Now we can finally grab our first batch and set our initial time to calculate how long our iterations are taking!"
      ],
      "metadata": {
        "id": "Nhqmxeo0Eg0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch('train')\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model\n",
        "running_mfu = -1.0 # model flops utilization\n",
        "\n",
        "while True:\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        # get loss as float. note: this is a CPU-GPU sync point\n",
        "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHbyEapRWmpc",
        "outputId": "baa91af0-33ce-4616-c28a-24b0d2db7c71"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 9.9352, val loss 9.9273\n",
            "iter 0: loss 9.9333, time 91252.99ms, mfu -100.00%\n",
            "iter 10: loss 8.3523, time 204.32ms, mfu 2.52%\n",
            "iter 20: loss 7.3770, time 204.90ms, mfu 2.52%\n",
            "iter 30: loss 6.4399, time 207.78ms, mfu 2.52%\n",
            "iter 40: loss 5.8015, time 205.73ms, mfu 2.52%\n",
            "iter 50: loss 5.7303, time 207.39ms, mfu 2.51%\n",
            "iter 60: loss 5.5225, time 206.59ms, mfu 2.51%\n",
            "iter 70: loss 5.2348, time 207.68ms, mfu 2.51%\n",
            "iter 80: loss 5.1082, time 207.49ms, mfu 2.51%\n",
            "iter 90: loss 4.9919, time 207.92ms, mfu 2.50%\n",
            "iter 100: loss 4.5894, time 208.46ms, mfu 2.50%\n",
            "iter 110: loss 4.6089, time 209.18ms, mfu 2.50%\n",
            "iter 120: loss 4.5490, time 208.90ms, mfu 2.49%\n",
            "iter 130: loss 4.5350, time 209.30ms, mfu 2.49%\n",
            "iter 140: loss 4.4533, time 210.20ms, mfu 2.49%\n",
            "iter 150: loss 4.4550, time 210.32ms, mfu 2.48%\n",
            "iter 160: loss 4.4424, time 211.71ms, mfu 2.48%\n",
            "iter 170: loss 4.3052, time 212.89ms, mfu 2.47%\n",
            "iter 180: loss 4.4285, time 211.63ms, mfu 2.47%\n",
            "iter 190: loss 4.2655, time 211.83ms, mfu 2.47%\n",
            "iter 200: loss 4.2285, time 213.46ms, mfu 2.46%\n",
            "iter 210: loss 4.2508, time 217.22ms, mfu 2.45%\n",
            "iter 220: loss 4.2502, time 214.28ms, mfu 2.45%\n",
            "iter 230: loss 4.1954, time 216.23ms, mfu 2.44%\n",
            "iter 240: loss 4.0983, time 213.52ms, mfu 2.44%\n",
            "step 250: train loss 3.9783, val loss 4.9502\n",
            "saving checkpoint to out\n",
            "iter 250: loss 4.0320, time 22967.57ms, mfu 2.20%\n",
            "iter 260: loss 4.0705, time 209.19ms, mfu 2.22%\n",
            "iter 270: loss 3.8854, time 213.43ms, mfu 2.24%\n",
            "iter 280: loss 4.0231, time 210.35ms, mfu 2.26%\n",
            "iter 290: loss 3.8797, time 210.30ms, mfu 2.28%\n",
            "iter 300: loss 3.8969, time 211.72ms, mfu 2.30%\n",
            "iter 310: loss 3.9811, time 212.34ms, mfu 2.31%\n",
            "iter 320: loss 3.7998, time 210.89ms, mfu 2.32%\n",
            "iter 330: loss 3.8033, time 209.85ms, mfu 2.34%\n",
            "iter 340: loss 3.8841, time 210.69ms, mfu 2.35%\n",
            "iter 350: loss 3.7314, time 210.91ms, mfu 2.36%\n",
            "iter 360: loss 3.6185, time 211.76ms, mfu 2.37%\n",
            "iter 370: loss 3.7398, time 213.29ms, mfu 2.37%\n",
            "iter 380: loss 3.7080, time 212.47ms, mfu 2.38%\n",
            "iter 390: loss 3.6893, time 213.32ms, mfu 2.38%\n",
            "iter 400: loss 3.4831, time 213.41ms, mfu 2.38%\n",
            "iter 410: loss 3.6749, time 213.19ms, mfu 2.39%\n",
            "iter 420: loss 3.5365, time 214.94ms, mfu 2.39%\n",
            "iter 430: loss 3.6374, time 215.66ms, mfu 2.39%\n",
            "iter 440: loss 3.5321, time 215.63ms, mfu 2.39%\n",
            "iter 450: loss 3.5467, time 214.91ms, mfu 2.39%\n",
            "iter 460: loss 3.5017, time 217.00ms, mfu 2.39%\n",
            "iter 470: loss 3.5595, time 216.92ms, mfu 2.39%\n",
            "iter 480: loss 3.3411, time 215.23ms, mfu 2.39%\n",
            "iter 490: loss 3.4868, time 214.80ms, mfu 2.39%\n",
            "step 500: train loss 3.3528, val loss 5.1432\n",
            "saving checkpoint to out\n",
            "iter 500: loss 3.3810, time 26555.97ms, mfu 2.15%\n",
            "iter 510: loss 3.5480, time 213.39ms, mfu 2.18%\n",
            "iter 520: loss 3.4318, time 211.52ms, mfu 2.20%\n",
            "iter 530: loss 3.3128, time 212.69ms, mfu 2.23%\n",
            "iter 540: loss 3.4681, time 213.42ms, mfu 2.24%\n",
            "iter 550: loss 3.2918, time 213.11ms, mfu 2.26%\n",
            "iter 560: loss 3.1920, time 212.89ms, mfu 2.28%\n",
            "iter 570: loss 3.1144, time 210.83ms, mfu 2.29%\n",
            "iter 580: loss 3.2399, time 211.85ms, mfu 2.31%\n",
            "iter 590: loss 3.1596, time 212.20ms, mfu 2.32%\n",
            "iter 600: loss 3.2114, time 211.81ms, mfu 2.33%\n",
            "iter 610: loss 3.1560, time 212.66ms, mfu 2.34%\n",
            "iter 620: loss 3.1159, time 213.18ms, mfu 2.35%\n",
            "iter 630: loss 3.2241, time 211.26ms, mfu 2.36%\n",
            "iter 640: loss 3.2018, time 212.38ms, mfu 2.36%\n",
            "iter 650: loss 3.1171, time 212.80ms, mfu 2.37%\n",
            "iter 660: loss 3.1302, time 214.30ms, mfu 2.37%\n",
            "iter 670: loss 3.1158, time 215.45ms, mfu 2.38%\n",
            "iter 680: loss 2.9843, time 216.36ms, mfu 2.38%\n",
            "iter 690: loss 3.0538, time 214.61ms, mfu 2.38%\n",
            "iter 700: loss 2.9401, time 216.87ms, mfu 2.38%\n",
            "iter 710: loss 3.0146, time 213.72ms, mfu 2.38%\n",
            "iter 720: loss 2.7830, time 218.41ms, mfu 2.38%\n",
            "iter 730: loss 2.9433, time 214.04ms, mfu 2.38%\n",
            "iter 740: loss 2.8851, time 214.75ms, mfu 2.38%\n",
            "step 750: train loss 2.6819, val loss 5.4476\n",
            "saving checkpoint to out\n",
            "iter 750: loss 2.8290, time 22666.98ms, mfu 2.15%\n",
            "iter 760: loss 2.8452, time 215.26ms, mfu 2.17%\n",
            "iter 770: loss 2.7423, time 211.06ms, mfu 2.20%\n",
            "iter 780: loss 2.7535, time 210.52ms, mfu 2.22%\n",
            "iter 790: loss 2.6695, time 212.46ms, mfu 2.24%\n",
            "iter 800: loss 2.9788, time 213.02ms, mfu 2.26%\n",
            "iter 810: loss 2.7881, time 213.04ms, mfu 2.28%\n",
            "iter 820: loss 2.7613, time 213.40ms, mfu 2.29%\n",
            "iter 830: loss 2.6269, time 214.03ms, mfu 2.30%\n",
            "iter 840: loss 2.6770, time 212.27ms, mfu 2.32%\n",
            "iter 850: loss 2.8810, time 214.29ms, mfu 2.32%\n",
            "iter 860: loss 2.6417, time 213.17ms, mfu 2.33%\n",
            "iter 870: loss 2.5154, time 213.29ms, mfu 2.34%\n",
            "iter 880: loss 2.5561, time 215.90ms, mfu 2.35%\n",
            "iter 890: loss 2.5065, time 214.25ms, mfu 2.35%\n",
            "iter 900: loss 2.4186, time 214.04ms, mfu 2.36%\n",
            "iter 910: loss 2.5570, time 214.48ms, mfu 2.36%\n",
            "iter 920: loss 2.4896, time 214.08ms, mfu 2.37%\n",
            "iter 930: loss 2.3588, time 212.04ms, mfu 2.37%\n",
            "iter 940: loss 2.5304, time 214.99ms, mfu 2.38%\n",
            "iter 950: loss 2.4135, time 221.88ms, mfu 2.37%\n",
            "iter 960: loss 2.1213, time 212.87ms, mfu 2.38%\n",
            "iter 970: loss 2.2166, time 214.78ms, mfu 2.38%\n",
            "iter 980: loss 2.2643, time 213.54ms, mfu 2.38%\n",
            "iter 990: loss 2.1437, time 221.89ms, mfu 2.38%\n",
            "step 1000: train loss 1.9328, val loss 5.9032\n",
            "saving checkpoint to out\n",
            "iter 1000: loss 2.4476, time 22775.92ms, mfu 2.14%\n",
            "iter 1010: loss 2.2166, time 216.05ms, mfu 2.17%\n",
            "iter 1020: loss 2.2759, time 215.94ms, mfu 2.19%\n",
            "iter 1030: loss 2.3462, time 213.94ms, mfu 2.21%\n",
            "iter 1040: loss 1.9942, time 214.18ms, mfu 2.23%\n",
            "iter 1050: loss 1.9840, time 212.29ms, mfu 2.25%\n",
            "iter 1060: loss 2.0996, time 215.74ms, mfu 2.26%\n",
            "iter 1070: loss 1.9666, time 213.39ms, mfu 2.28%\n",
            "iter 1080: loss 2.1540, time 214.37ms, mfu 2.29%\n",
            "iter 1090: loss 1.8240, time 214.61ms, mfu 2.30%\n",
            "iter 1100: loss 2.0257, time 213.73ms, mfu 2.31%\n",
            "iter 1110: loss 1.9974, time 214.97ms, mfu 2.32%\n",
            "iter 1120: loss 1.9280, time 214.30ms, mfu 2.33%\n",
            "iter 1130: loss 1.8272, time 214.77ms, mfu 2.34%\n",
            "iter 1140: loss 1.8513, time 214.93ms, mfu 2.34%\n",
            "iter 1150: loss 1.9047, time 212.53ms, mfu 2.35%\n",
            "iter 1160: loss 1.9818, time 214.61ms, mfu 2.36%\n",
            "iter 1170: loss 1.9431, time 212.75ms, mfu 2.36%\n",
            "iter 1180: loss 1.7304, time 214.45ms, mfu 2.37%\n",
            "iter 1190: loss 1.9152, time 211.01ms, mfu 2.37%\n",
            "iter 1200: loss 1.8428, time 214.34ms, mfu 2.38%\n",
            "iter 1210: loss 1.9158, time 215.24ms, mfu 2.38%\n",
            "iter 1220: loss 1.8026, time 214.56ms, mfu 2.38%\n",
            "iter 1230: loss 1.7657, time 212.97ms, mfu 2.39%\n",
            "iter 1240: loss 1.6764, time 214.17ms, mfu 2.39%\n",
            "step 1250: train loss 1.3073, val loss 6.4031\n",
            "saving checkpoint to out\n",
            "iter 1250: loss 1.6578, time 23281.57ms, mfu 2.15%\n",
            "iter 1260: loss 1.6763, time 214.96ms, mfu 2.18%\n",
            "iter 1270: loss 1.7930, time 216.21ms, mfu 2.20%\n",
            "iter 1280: loss 1.5780, time 215.02ms, mfu 2.22%\n",
            "iter 1290: loss 1.5957, time 214.03ms, mfu 2.24%\n",
            "iter 1300: loss 1.6626, time 213.60ms, mfu 2.25%\n",
            "iter 1310: loss 1.6653, time 214.78ms, mfu 2.27%\n",
            "iter 1320: loss 1.5428, time 215.57ms, mfu 2.28%\n",
            "iter 1330: loss 1.5060, time 214.26ms, mfu 2.29%\n",
            "iter 1340: loss 1.4959, time 214.73ms, mfu 2.30%\n",
            "iter 1350: loss 1.6159, time 213.39ms, mfu 2.31%\n",
            "iter 1360: loss 1.6263, time 213.87ms, mfu 2.32%\n",
            "iter 1370: loss 1.4958, time 213.54ms, mfu 2.33%\n",
            "iter 1380: loss 1.5586, time 215.53ms, mfu 2.34%\n",
            "iter 1390: loss 1.5384, time 212.68ms, mfu 2.35%\n",
            "iter 1400: loss 1.4564, time 214.98ms, mfu 2.35%\n",
            "iter 1410: loss 1.4374, time 214.20ms, mfu 2.36%\n",
            "iter 1420: loss 1.4928, time 214.31ms, mfu 2.36%\n",
            "iter 1430: loss 1.4613, time 213.10ms, mfu 2.37%\n",
            "iter 1440: loss 1.3919, time 214.51ms, mfu 2.37%\n",
            "iter 1450: loss 1.5591, time 213.03ms, mfu 2.38%\n",
            "iter 1460: loss 1.3262, time 212.85ms, mfu 2.38%\n",
            "iter 1470: loss 1.4623, time 214.82ms, mfu 2.38%\n",
            "iter 1480: loss 1.3243, time 215.48ms, mfu 2.38%\n",
            "iter 1490: loss 1.4489, time 214.85ms, mfu 2.39%\n",
            "step 1500: train loss 0.9134, val loss 6.8978\n",
            "saving checkpoint to out\n",
            "iter 1500: loss 1.3850, time 27694.40ms, mfu 2.15%\n",
            "iter 1510: loss 1.3102, time 213.03ms, mfu 2.18%\n",
            "iter 1520: loss 1.4021, time 210.90ms, mfu 2.20%\n",
            "iter 1530: loss 1.2306, time 213.70ms, mfu 2.22%\n",
            "iter 1540: loss 1.2885, time 212.92ms, mfu 2.24%\n",
            "iter 1550: loss 1.2552, time 214.27ms, mfu 2.26%\n",
            "iter 1560: loss 1.2244, time 213.68ms, mfu 2.27%\n",
            "iter 1570: loss 1.2563, time 215.00ms, mfu 2.29%\n",
            "iter 1580: loss 1.2424, time 215.65ms, mfu 2.30%\n",
            "iter 1590: loss 1.2099, time 214.62ms, mfu 2.31%\n",
            "iter 1600: loss 1.2510, time 214.16ms, mfu 2.32%\n",
            "iter 1610: loss 1.2787, time 214.59ms, mfu 2.33%\n",
            "iter 1620: loss 1.2344, time 215.24ms, mfu 2.33%\n",
            "iter 1630: loss 1.2147, time 214.94ms, mfu 2.34%\n",
            "iter 1640: loss 1.1499, time 214.77ms, mfu 2.35%\n",
            "iter 1650: loss 1.1825, time 215.02ms, mfu 2.35%\n",
            "iter 1660: loss 1.1459, time 217.05ms, mfu 2.35%\n",
            "iter 1670: loss 1.1410, time 214.81ms, mfu 2.36%\n",
            "iter 1680: loss 1.1493, time 213.64ms, mfu 2.36%\n",
            "iter 1690: loss 1.2434, time 214.16ms, mfu 2.37%\n",
            "iter 1700: loss 1.1194, time 214.17ms, mfu 2.37%\n",
            "iter 1710: loss 1.0185, time 213.90ms, mfu 2.38%\n",
            "iter 1720: loss 1.0891, time 213.56ms, mfu 2.38%\n",
            "iter 1730: loss 1.1044, time 212.20ms, mfu 2.38%\n",
            "iter 1740: loss 1.0714, time 216.20ms, mfu 2.38%\n",
            "step 1750: train loss 0.6764, val loss 7.2460\n",
            "saving checkpoint to out\n",
            "iter 1750: loss 1.0785, time 27349.85ms, mfu 2.15%\n",
            "iter 1760: loss 1.0767, time 211.17ms, mfu 2.18%\n",
            "iter 1770: loss 1.0777, time 213.22ms, mfu 2.20%\n",
            "iter 1780: loss 1.0318, time 212.38ms, mfu 2.22%\n",
            "iter 1790: loss 1.1314, time 212.53ms, mfu 2.24%\n",
            "iter 1800: loss 1.1167, time 214.11ms, mfu 2.26%\n",
            "iter 1810: loss 0.9832, time 214.69ms, mfu 2.27%\n",
            "iter 1820: loss 1.0768, time 215.47ms, mfu 2.29%\n",
            "iter 1830: loss 1.0038, time 215.33ms, mfu 2.30%\n",
            "iter 1840: loss 1.0437, time 214.91ms, mfu 2.31%\n",
            "iter 1850: loss 0.9971, time 213.07ms, mfu 2.32%\n",
            "iter 1860: loss 1.0093, time 215.08ms, mfu 2.33%\n",
            "iter 1870: loss 0.9026, time 219.90ms, mfu 2.33%\n",
            "iter 1880: loss 1.0012, time 217.42ms, mfu 2.33%\n",
            "iter 1890: loss 0.9739, time 215.42ms, mfu 2.34%\n",
            "iter 1900: loss 0.9945, time 215.44ms, mfu 2.34%\n",
            "iter 1910: loss 0.9338, time 216.10ms, mfu 2.35%\n",
            "iter 1920: loss 1.0085, time 215.36ms, mfu 2.35%\n",
            "iter 1930: loss 0.9597, time 214.38ms, mfu 2.36%\n",
            "iter 1940: loss 1.0024, time 213.00ms, mfu 2.36%\n",
            "iter 1950: loss 0.9211, time 215.35ms, mfu 2.37%\n",
            "iter 1960: loss 0.9562, time 213.88ms, mfu 2.37%\n",
            "iter 1970: loss 0.9323, time 215.12ms, mfu 2.37%\n",
            "iter 1980: loss 0.9304, time 213.05ms, mfu 2.38%\n",
            "iter 1990: loss 0.9054, time 214.48ms, mfu 2.38%\n",
            "step 2000: train loss 0.5097, val loss 7.6423\n",
            "saving checkpoint to out\n",
            "iter 2000: loss 0.9064, time 27338.90ms, mfu 2.14%\n",
            "iter 2010: loss 0.9271, time 209.66ms, mfu 2.18%\n",
            "iter 2020: loss 0.9085, time 211.84ms, mfu 2.20%\n",
            "iter 2030: loss 0.8328, time 213.29ms, mfu 2.22%\n",
            "iter 2040: loss 0.8003, time 213.35ms, mfu 2.24%\n",
            "iter 2050: loss 0.8490, time 211.89ms, mfu 2.26%\n",
            "iter 2060: loss 0.8638, time 214.49ms, mfu 2.28%\n",
            "iter 2070: loss 0.8208, time 212.06ms, mfu 2.29%\n",
            "iter 2080: loss 0.8364, time 212.93ms, mfu 2.30%\n",
            "iter 2090: loss 0.8070, time 214.63ms, mfu 2.31%\n",
            "iter 2100: loss 0.8457, time 216.39ms, mfu 2.32%\n",
            "iter 2110: loss 0.8635, time 213.99ms, mfu 2.33%\n",
            "iter 2120: loss 0.8796, time 214.53ms, mfu 2.34%\n",
            "iter 2130: loss 0.8430, time 217.45ms, mfu 2.34%\n",
            "iter 2140: loss 0.8700, time 215.89ms, mfu 2.34%\n",
            "iter 2150: loss 0.7377, time 213.88ms, mfu 2.35%\n",
            "iter 2160: loss 0.7796, time 215.82ms, mfu 2.36%\n",
            "iter 2170: loss 0.8803, time 216.61ms, mfu 2.36%\n",
            "iter 2180: loss 0.8533, time 215.30ms, mfu 2.36%\n",
            "iter 2190: loss 0.8047, time 216.15ms, mfu 2.36%\n",
            "iter 2200: loss 0.7583, time 213.59ms, mfu 2.37%\n",
            "iter 2210: loss 0.7421, time 215.33ms, mfu 2.37%\n",
            "iter 2220: loss 0.7806, time 214.25ms, mfu 2.37%\n",
            "iter 2230: loss 0.7440, time 214.44ms, mfu 2.38%\n",
            "iter 2240: loss 0.7429, time 211.98ms, mfu 2.38%\n",
            "step 2250: train loss 0.3954, val loss 7.8678\n",
            "saving checkpoint to out\n",
            "iter 2250: loss 0.7594, time 27399.94ms, mfu 2.15%\n",
            "iter 2260: loss 0.7424, time 211.23ms, mfu 2.18%\n",
            "iter 2270: loss 0.6985, time 211.28ms, mfu 2.20%\n",
            "iter 2280: loss 0.6870, time 211.37ms, mfu 2.23%\n",
            "iter 2290: loss 0.7267, time 211.71ms, mfu 2.25%\n",
            "iter 2300: loss 0.7520, time 213.25ms, mfu 2.26%\n",
            "iter 2310: loss 0.6993, time 212.38ms, mfu 2.28%\n",
            "iter 2320: loss 0.7133, time 214.11ms, mfu 2.29%\n",
            "iter 2330: loss 0.7076, time 213.66ms, mfu 2.30%\n",
            "iter 2340: loss 0.7332, time 213.62ms, mfu 2.32%\n",
            "iter 2350: loss 0.6996, time 214.84ms, mfu 2.32%\n",
            "iter 2360: loss 0.7223, time 215.04ms, mfu 2.33%\n",
            "iter 2370: loss 0.7090, time 215.36ms, mfu 2.34%\n",
            "iter 2380: loss 0.6758, time 213.95ms, mfu 2.34%\n",
            "iter 2390: loss 0.7106, time 216.85ms, mfu 2.35%\n",
            "iter 2400: loss 0.6632, time 214.84ms, mfu 2.35%\n",
            "iter 2410: loss 0.7914, time 217.08ms, mfu 2.36%\n",
            "iter 2420: loss 0.6558, time 214.67ms, mfu 2.36%\n",
            "iter 2430: loss 0.6176, time 216.26ms, mfu 2.36%\n",
            "iter 2440: loss 0.6856, time 216.07ms, mfu 2.36%\n",
            "iter 2450: loss 0.6515, time 215.05ms, mfu 2.37%\n",
            "iter 2460: loss 0.5888, time 215.59ms, mfu 2.37%\n",
            "iter 2470: loss 0.6688, time 214.60ms, mfu 2.37%\n",
            "iter 2480: loss 0.6756, time 212.68ms, mfu 2.38%\n",
            "iter 2490: loss 0.6337, time 214.82ms, mfu 2.38%\n",
            "step 2500: train loss 0.3080, val loss 8.1598\n",
            "saving checkpoint to out\n",
            "iter 2500: loss 0.6508, time 26815.60ms, mfu 2.14%\n",
            "iter 2510: loss 0.6082, time 212.54ms, mfu 2.17%\n",
            "iter 2520: loss 0.6285, time 212.73ms, mfu 2.20%\n",
            "iter 2530: loss 0.5648, time 212.13ms, mfu 2.22%\n",
            "iter 2540: loss 0.6829, time 214.11ms, mfu 2.24%\n",
            "iter 2550: loss 0.5987, time 214.02ms, mfu 2.26%\n",
            "iter 2560: loss 0.5971, time 212.94ms, mfu 2.27%\n",
            "iter 2570: loss 0.6388, time 213.81ms, mfu 2.29%\n",
            "iter 2580: loss 0.6362, time 214.29ms, mfu 2.30%\n",
            "iter 2590: loss 0.5867, time 213.88ms, mfu 2.31%\n",
            "iter 2600: loss 0.5864, time 214.54ms, mfu 2.32%\n",
            "iter 2610: loss 0.6114, time 216.42ms, mfu 2.33%\n",
            "iter 2620: loss 0.5958, time 214.59ms, mfu 2.33%\n",
            "iter 2630: loss 0.5968, time 214.43ms, mfu 2.34%\n",
            "iter 2640: loss 0.5790, time 214.75ms, mfu 2.35%\n",
            "iter 2650: loss 0.5790, time 215.29ms, mfu 2.35%\n",
            "iter 2660: loss 0.5794, time 215.64ms, mfu 2.35%\n",
            "iter 2670: loss 0.5502, time 215.76ms, mfu 2.36%\n",
            "iter 2680: loss 0.6198, time 212.82ms, mfu 2.36%\n",
            "iter 2690: loss 0.6049, time 215.62ms, mfu 2.37%\n",
            "iter 2700: loss 0.5460, time 215.73ms, mfu 2.37%\n",
            "iter 2710: loss 0.5575, time 215.72ms, mfu 2.37%\n",
            "iter 2720: loss 0.5470, time 215.53ms, mfu 2.37%\n",
            "iter 2730: loss 0.5100, time 213.84ms, mfu 2.38%\n",
            "iter 2740: loss 0.5081, time 214.09ms, mfu 2.38%\n",
            "step 2750: train loss 0.2360, val loss 8.3525\n",
            "saving checkpoint to out\n",
            "iter 2750: loss 0.5711, time 27383.36ms, mfu 2.14%\n",
            "iter 2760: loss 0.5862, time 212.95ms, mfu 2.17%\n",
            "iter 2770: loss 0.5058, time 212.07ms, mfu 2.20%\n",
            "iter 2780: loss 0.5555, time 211.50ms, mfu 2.22%\n",
            "iter 2790: loss 0.4848, time 211.74ms, mfu 2.24%\n",
            "iter 2800: loss 0.5143, time 212.18ms, mfu 2.26%\n",
            "iter 2810: loss 0.5475, time 214.69ms, mfu 2.28%\n",
            "iter 2820: loss 0.4801, time 211.50ms, mfu 2.29%\n",
            "iter 2830: loss 0.4743, time 214.50ms, mfu 2.30%\n",
            "iter 2840: loss 0.5318, time 213.67ms, mfu 2.31%\n",
            "iter 2850: loss 0.4730, time 214.71ms, mfu 2.32%\n",
            "iter 2860: loss 0.4544, time 215.88ms, mfu 2.33%\n",
            "iter 2870: loss 0.4920, time 214.63ms, mfu 2.34%\n",
            "iter 2880: loss 0.4601, time 214.61ms, mfu 2.34%\n",
            "iter 2890: loss 0.4595, time 214.21ms, mfu 2.35%\n",
            "iter 2900: loss 0.4589, time 215.26ms, mfu 2.35%\n",
            "iter 2910: loss 0.4136, time 215.55ms, mfu 2.36%\n",
            "iter 2920: loss 0.4619, time 214.47ms, mfu 2.36%\n",
            "iter 2930: loss 0.4865, time 215.17ms, mfu 2.37%\n",
            "iter 2940: loss 0.4628, time 215.25ms, mfu 2.37%\n",
            "iter 2950: loss 0.4795, time 214.50ms, mfu 2.37%\n",
            "iter 2960: loss 0.4573, time 214.19ms, mfu 2.38%\n",
            "iter 2970: loss 0.4499, time 213.70ms, mfu 2.38%\n",
            "iter 2980: loss 0.4490, time 214.34ms, mfu 2.38%\n",
            "iter 2990: loss 0.4533, time 215.16ms, mfu 2.38%\n",
            "step 3000: train loss 0.1914, val loss 8.5183\n",
            "saving checkpoint to out\n",
            "iter 3000: loss 0.4537, time 27353.74ms, mfu 2.15%\n",
            "iter 3010: loss 0.4657, time 211.87ms, mfu 2.18%\n",
            "iter 3020: loss 0.4466, time 212.62ms, mfu 2.20%\n",
            "iter 3030: loss 0.4303, time 210.96ms, mfu 2.22%\n",
            "iter 3040: loss 0.4692, time 211.98ms, mfu 2.25%\n",
            "iter 3050: loss 0.4374, time 213.55ms, mfu 2.26%\n",
            "iter 3060: loss 0.4181, time 212.31ms, mfu 2.28%\n",
            "iter 3070: loss 0.4175, time 213.05ms, mfu 2.29%\n",
            "iter 3080: loss 0.4181, time 213.91ms, mfu 2.30%\n",
            "iter 3090: loss 0.3924, time 213.12ms, mfu 2.32%\n",
            "iter 3100: loss 0.4326, time 213.50ms, mfu 2.33%\n",
            "iter 3110: loss 0.4500, time 214.75ms, mfu 2.33%\n",
            "iter 3120: loss 0.4461, time 214.56ms, mfu 2.34%\n",
            "iter 3130: loss 0.4489, time 215.28ms, mfu 2.35%\n",
            "iter 3140: loss 0.4437, time 215.61ms, mfu 2.35%\n",
            "iter 3150: loss 0.4364, time 213.60ms, mfu 2.36%\n",
            "iter 3160: loss 0.3973, time 215.30ms, mfu 2.36%\n",
            "iter 3170: loss 0.4262, time 216.54ms, mfu 2.36%\n",
            "iter 3180: loss 0.3853, time 216.52ms, mfu 2.36%\n",
            "iter 3190: loss 0.3871, time 214.07ms, mfu 2.37%\n",
            "iter 3200: loss 0.4177, time 213.89ms, mfu 2.37%\n",
            "iter 3210: loss 0.4438, time 214.20ms, mfu 2.38%\n",
            "iter 3220: loss 0.3375, time 213.70ms, mfu 2.38%\n",
            "iter 3230: loss 0.4309, time 214.36ms, mfu 2.38%\n",
            "iter 3240: loss 0.3849, time 213.68ms, mfu 2.38%\n",
            "step 3250: train loss 0.1541, val loss 8.6594\n",
            "saving checkpoint to out\n",
            "iter 3250: loss 0.4173, time 26045.75ms, mfu 2.15%\n",
            "iter 3260: loss 0.3616, time 210.88ms, mfu 2.18%\n",
            "iter 3270: loss 0.3498, time 212.53ms, mfu 2.20%\n",
            "iter 3280: loss 0.3741, time 212.77ms, mfu 2.22%\n",
            "iter 3290: loss 0.3772, time 214.30ms, mfu 2.24%\n",
            "iter 3300: loss 0.3880, time 210.31ms, mfu 2.26%\n",
            "iter 3310: loss 0.3758, time 212.22ms, mfu 2.28%\n",
            "iter 3320: loss 0.3774, time 213.62ms, mfu 2.29%\n",
            "iter 3330: loss 0.3778, time 214.32ms, mfu 2.30%\n",
            "iter 3340: loss 0.3881, time 215.04ms, mfu 2.31%\n",
            "iter 3350: loss 0.3835, time 213.33ms, mfu 2.32%\n",
            "iter 3360: loss 0.3733, time 214.94ms, mfu 2.33%\n",
            "iter 3370: loss 0.3848, time 213.21ms, mfu 2.34%\n",
            "iter 3380: loss 0.3866, time 214.16ms, mfu 2.35%\n",
            "iter 3390: loss 0.3668, time 214.33ms, mfu 2.35%\n",
            "iter 3400: loss 0.3327, time 214.23ms, mfu 2.36%\n",
            "iter 3410: loss 0.3776, time 214.08ms, mfu 2.36%\n",
            "iter 3420: loss 0.3618, time 214.05ms, mfu 2.37%\n",
            "iter 3430: loss 0.3543, time 214.75ms, mfu 2.37%\n",
            "iter 3440: loss 0.3403, time 215.50ms, mfu 2.37%\n",
            "iter 3450: loss 0.3417, time 215.33ms, mfu 2.37%\n",
            "iter 3460: loss 0.3604, time 214.03ms, mfu 2.38%\n",
            "iter 3470: loss 0.3282, time 213.68ms, mfu 2.38%\n",
            "iter 3480: loss 0.3276, time 214.32ms, mfu 2.38%\n",
            "iter 3490: loss 0.3540, time 214.20ms, mfu 2.39%\n",
            "step 3500: train loss 0.1300, val loss 8.8386\n",
            "saving checkpoint to out\n",
            "iter 3500: loss 0.3056, time 27422.41ms, mfu 2.15%\n",
            "iter 3510: loss 0.3223, time 212.80ms, mfu 2.18%\n",
            "iter 3520: loss 0.3632, time 212.11ms, mfu 2.20%\n",
            "iter 3530: loss 0.2874, time 212.32ms, mfu 2.22%\n",
            "iter 3540: loss 0.3166, time 214.14ms, mfu 2.24%\n",
            "iter 3550: loss 0.3130, time 212.51ms, mfu 2.26%\n",
            "iter 3560: loss 0.3385, time 211.99ms, mfu 2.28%\n",
            "iter 3570: loss 0.3156, time 212.42ms, mfu 2.29%\n",
            "iter 3580: loss 0.3360, time 214.45ms, mfu 2.30%\n",
            "iter 3590: loss 0.3119, time 214.80ms, mfu 2.31%\n",
            "iter 3600: loss 0.3221, time 215.53ms, mfu 2.32%\n",
            "iter 3610: loss 0.3001, time 214.87ms, mfu 2.33%\n",
            "iter 3620: loss 0.3232, time 216.84ms, mfu 2.33%\n",
            "iter 3630: loss 0.3098, time 213.81ms, mfu 2.34%\n",
            "iter 3640: loss 0.3150, time 215.18ms, mfu 2.35%\n",
            "iter 3650: loss 0.3095, time 217.00ms, mfu 2.35%\n",
            "iter 3660: loss 0.2825, time 215.07ms, mfu 2.35%\n",
            "iter 3670: loss 0.2917, time 215.05ms, mfu 2.36%\n",
            "iter 3680: loss 0.3167, time 215.61ms, mfu 2.36%\n",
            "iter 3690: loss 0.3108, time 214.50ms, mfu 2.37%\n",
            "iter 3700: loss 0.2917, time 212.50ms, mfu 2.37%\n",
            "iter 3710: loss 0.2831, time 215.31ms, mfu 2.37%\n",
            "iter 3720: loss 0.2653, time 213.88ms, mfu 2.38%\n",
            "iter 3730: loss 0.2901, time 214.14ms, mfu 2.38%\n",
            "iter 3740: loss 0.2991, time 213.35ms, mfu 2.38%\n",
            "step 3750: train loss 0.1095, val loss 8.8550\n",
            "saving checkpoint to out\n",
            "iter 3750: loss 0.3039, time 27353.69ms, mfu 2.15%\n",
            "iter 3760: loss 0.3155, time 213.87ms, mfu 2.17%\n",
            "iter 3770: loss 0.2915, time 211.62ms, mfu 2.20%\n",
            "iter 3780: loss 0.2845, time 212.75ms, mfu 2.22%\n",
            "iter 3790: loss 0.2933, time 211.91ms, mfu 2.24%\n",
            "iter 3800: loss 0.2778, time 214.22ms, mfu 2.26%\n",
            "iter 3810: loss 0.2856, time 212.42ms, mfu 2.28%\n",
            "iter 3820: loss 0.2792, time 216.07ms, mfu 2.29%\n",
            "iter 3830: loss 0.2867, time 211.86ms, mfu 2.30%\n",
            "iter 3840: loss 0.2728, time 213.78ms, mfu 2.31%\n",
            "iter 3850: loss 0.2755, time 215.10ms, mfu 2.32%\n",
            "iter 3860: loss 0.2501, time 214.06ms, mfu 2.33%\n",
            "iter 3870: loss 0.2755, time 215.23ms, mfu 2.34%\n",
            "iter 3880: loss 0.2705, time 215.53ms, mfu 2.34%\n",
            "iter 3890: loss 0.2762, time 216.46ms, mfu 2.35%\n",
            "iter 3900: loss 0.2686, time 215.68ms, mfu 2.35%\n",
            "iter 3910: loss 0.2704, time 213.16ms, mfu 2.36%\n",
            "iter 3920: loss 0.2692, time 216.30ms, mfu 2.36%\n",
            "iter 3930: loss 0.2502, time 215.86ms, mfu 2.36%\n",
            "iter 3940: loss 0.2654, time 214.58ms, mfu 2.37%\n",
            "iter 3950: loss 0.2603, time 216.46ms, mfu 2.37%\n",
            "iter 3960: loss 0.2747, time 213.60ms, mfu 2.37%\n",
            "iter 3970: loss 0.2524, time 215.02ms, mfu 2.37%\n",
            "iter 3980: loss 0.2386, time 212.94ms, mfu 2.38%\n",
            "iter 3990: loss 0.2397, time 213.32ms, mfu 2.38%\n",
            "step 4000: train loss 0.0981, val loss 8.9754\n",
            "saving checkpoint to out\n",
            "iter 4000: loss 0.2712, time 26116.14ms, mfu 2.15%\n",
            "iter 4010: loss 0.2561, time 212.84ms, mfu 2.17%\n",
            "iter 4020: loss 0.2636, time 212.02ms, mfu 2.20%\n",
            "iter 4030: loss 0.2590, time 214.12ms, mfu 2.22%\n",
            "iter 4040: loss 0.2438, time 214.78ms, mfu 2.24%\n",
            "iter 4050: loss 0.2543, time 212.73ms, mfu 2.26%\n",
            "iter 4060: loss 0.2506, time 214.62ms, mfu 2.27%\n",
            "iter 4070: loss 0.2480, time 214.17ms, mfu 2.29%\n",
            "iter 4080: loss 0.2510, time 212.97ms, mfu 2.30%\n",
            "iter 4090: loss 0.2626, time 214.60ms, mfu 2.31%\n",
            "iter 4100: loss 0.2590, time 213.82ms, mfu 2.32%\n",
            "iter 4110: loss 0.2299, time 213.31ms, mfu 2.33%\n",
            "iter 4120: loss 0.2661, time 214.25ms, mfu 2.34%\n",
            "iter 4130: loss 0.2643, time 216.05ms, mfu 2.34%\n",
            "iter 4140: loss 0.2661, time 214.74ms, mfu 2.35%\n",
            "iter 4150: loss 0.2575, time 214.88ms, mfu 2.35%\n",
            "iter 4160: loss 0.2693, time 215.28ms, mfu 2.36%\n",
            "iter 4170: loss 0.2229, time 215.13ms, mfu 2.36%\n",
            "iter 4180: loss 0.2407, time 212.82ms, mfu 2.37%\n",
            "iter 4190: loss 0.2508, time 213.46ms, mfu 2.37%\n",
            "iter 4200: loss 0.2762, time 214.95ms, mfu 2.37%\n",
            "iter 4210: loss 0.2503, time 214.44ms, mfu 2.38%\n",
            "iter 4220: loss 0.2521, time 213.88ms, mfu 2.38%\n",
            "iter 4230: loss 0.2350, time 214.31ms, mfu 2.38%\n",
            "iter 4240: loss 0.2507, time 215.30ms, mfu 2.38%\n",
            "step 4250: train loss 0.0904, val loss 9.1041\n",
            "saving checkpoint to out\n",
            "iter 4250: loss 0.2423, time 27376.48ms, mfu 2.15%\n",
            "iter 4260: loss 0.2330, time 209.66ms, mfu 2.18%\n",
            "iter 4270: loss 0.2404, time 213.51ms, mfu 2.20%\n",
            "iter 4280: loss 0.2630, time 212.87ms, mfu 2.22%\n",
            "iter 4290: loss 0.2067, time 213.55ms, mfu 2.24%\n",
            "iter 4300: loss 0.2344, time 213.86ms, mfu 2.26%\n",
            "iter 4310: loss 0.2214, time 213.63ms, mfu 2.28%\n",
            "iter 4320: loss 0.2366, time 212.45ms, mfu 2.29%\n",
            "iter 4330: loss 0.2319, time 215.24ms, mfu 2.30%\n",
            "iter 4340: loss 0.2444, time 212.41ms, mfu 2.31%\n",
            "iter 4350: loss 0.2204, time 212.17ms, mfu 2.32%\n",
            "iter 4360: loss 0.2411, time 217.18ms, mfu 2.33%\n",
            "iter 4370: loss 0.2215, time 216.41ms, mfu 2.33%\n",
            "iter 4380: loss 0.1951, time 214.58ms, mfu 2.34%\n",
            "iter 4390: loss 0.2270, time 214.93ms, mfu 2.35%\n",
            "iter 4400: loss 0.2081, time 215.59ms, mfu 2.35%\n",
            "iter 4410: loss 0.2080, time 215.27ms, mfu 2.36%\n",
            "iter 4420: loss 0.2162, time 215.69ms, mfu 2.36%\n",
            "iter 4430: loss 0.2319, time 214.09ms, mfu 2.36%\n",
            "iter 4440: loss 0.2197, time 214.95ms, mfu 2.37%\n",
            "iter 4450: loss 0.2263, time 213.97ms, mfu 2.37%\n",
            "iter 4460: loss 0.2124, time 214.61ms, mfu 2.37%\n",
            "iter 4470: loss 0.2206, time 214.39ms, mfu 2.38%\n",
            "iter 4480: loss 0.2030, time 212.87ms, mfu 2.38%\n",
            "iter 4490: loss 0.2146, time 215.64ms, mfu 2.38%\n",
            "step 4500: train loss 0.0831, val loss 9.0696\n",
            "saving checkpoint to out\n",
            "iter 4500: loss 0.2394, time 27377.72ms, mfu 2.15%\n",
            "iter 4510: loss 0.2311, time 210.45ms, mfu 2.18%\n",
            "iter 4520: loss 0.2364, time 211.45ms, mfu 2.20%\n",
            "iter 4530: loss 0.2205, time 213.83ms, mfu 2.22%\n",
            "iter 4540: loss 0.2102, time 212.90ms, mfu 2.24%\n",
            "iter 4550: loss 0.2016, time 211.26ms, mfu 2.26%\n",
            "iter 4560: loss 0.2160, time 213.27ms, mfu 2.28%\n",
            "iter 4570: loss 0.2175, time 212.94ms, mfu 2.29%\n",
            "iter 4580: loss 0.2324, time 213.73ms, mfu 2.30%\n",
            "iter 4590: loss 0.2312, time 214.66ms, mfu 2.31%\n",
            "iter 4600: loss 0.2248, time 214.56ms, mfu 2.32%\n",
            "iter 4610: loss 0.2267, time 214.82ms, mfu 2.33%\n",
            "iter 4620: loss 0.2150, time 217.34ms, mfu 2.33%\n",
            "iter 4630: loss 0.2179, time 215.54ms, mfu 2.34%\n",
            "iter 4640: loss 0.1981, time 216.14ms, mfu 2.34%\n",
            "iter 4650: loss 0.2052, time 213.54ms, mfu 2.35%\n",
            "iter 4660: loss 0.2020, time 215.30ms, mfu 2.36%\n",
            "iter 4670: loss 0.2259, time 214.66ms, mfu 2.36%\n",
            "iter 4680: loss 0.2290, time 213.13ms, mfu 2.37%\n",
            "iter 4690: loss 0.2167, time 213.89ms, mfu 2.37%\n",
            "iter 4700: loss 0.2035, time 216.88ms, mfu 2.37%\n",
            "iter 4710: loss 0.2027, time 212.77ms, mfu 2.38%\n",
            "iter 4720: loss 0.1954, time 214.94ms, mfu 2.38%\n",
            "iter 4730: loss 0.2217, time 213.66ms, mfu 2.38%\n",
            "iter 4740: loss 0.2129, time 214.01ms, mfu 2.38%\n",
            "step 4750: train loss 0.0794, val loss 9.2162\n",
            "saving checkpoint to out\n",
            "iter 4750: loss 0.2016, time 26145.49ms, mfu 2.15%\n",
            "iter 4760: loss 0.2331, time 213.11ms, mfu 2.18%\n",
            "iter 4770: loss 0.2145, time 213.77ms, mfu 2.20%\n",
            "iter 4780: loss 0.1989, time 213.47ms, mfu 2.22%\n",
            "iter 4790: loss 0.1875, time 212.56ms, mfu 2.24%\n",
            "iter 4800: loss 0.2006, time 213.96ms, mfu 2.26%\n",
            "iter 4810: loss 0.2018, time 212.26ms, mfu 2.27%\n",
            "iter 4820: loss 0.2068, time 213.01ms, mfu 2.29%\n",
            "iter 4830: loss 0.2061, time 212.52ms, mfu 2.30%\n",
            "iter 4840: loss 0.1901, time 213.12ms, mfu 2.31%\n",
            "iter 4850: loss 0.1750, time 214.47ms, mfu 2.32%\n",
            "iter 4860: loss 0.2018, time 213.45ms, mfu 2.33%\n",
            "iter 4870: loss 0.1948, time 212.93ms, mfu 2.34%\n",
            "iter 4880: loss 0.1721, time 215.20ms, mfu 2.35%\n",
            "iter 4890: loss 0.2102, time 213.89ms, mfu 2.35%\n",
            "iter 4900: loss 0.1875, time 213.00ms, mfu 2.36%\n",
            "iter 4910: loss 0.2027, time 214.09ms, mfu 2.36%\n",
            "iter 4920: loss 0.2046, time 215.35ms, mfu 2.37%\n",
            "iter 4930: loss 0.1742, time 214.56ms, mfu 2.37%\n",
            "iter 4940: loss 0.1865, time 212.90ms, mfu 2.38%\n",
            "iter 4950: loss 0.1831, time 214.42ms, mfu 2.38%\n",
            "iter 4960: loss 0.1910, time 212.38ms, mfu 2.38%\n",
            "iter 4970: loss 0.1865, time 215.16ms, mfu 2.38%\n",
            "iter 4980: loss 0.2114, time 215.03ms, mfu 2.39%\n",
            "iter 4990: loss 0.1910, time 213.18ms, mfu 2.39%\n",
            "step 5000: train loss 0.0762, val loss 9.2574\n",
            "saving checkpoint to out\n",
            "iter 5000: loss 0.1863, time 27369.34ms, mfu 2.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Outputs with our New Model\n",
        "\n",
        "Now we can leverage the `sample.py` file to generate outputs from our model!"
      ],
      "metadata": {
        "id": "L2J5JlRxFJOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation Set Up and Model Loading"
      ],
      "metadata": {
        "id": "eo_QP1ITFfX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ],
      "metadata": {
        "id": "-vftqU9LheEK"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQRB3j7iiNkl",
        "outputId": "f198c292-6796-40cf-a5aa-f9ae06620b41"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 29.55M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
      ],
      "metadata": {
        "id": "N1YAy8DriVZZ"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tokenizer\n",
        "encode = lambda s: enc.encode(s)\n",
        "decode = lambda l: enc.decode(l)"
      ],
      "metadata": {
        "id": "KoB-5ZuLicAT"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation!"
      ],
      "metadata": {
        "id": "mkTQ9wo7FjYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmTcaHCjii5l",
        "outputId": "21e9674a-4edf-42f9-9fc0-98a334cfd51d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To the crown, in England's throne.\n",
            "\n",
            "ANGELO:\n",
            "Would I not that any of such hot?\n",
            "\n",
            "ISABELLA:\n",
            "Even so? why, I beseech you?\n",
            "\n",
            "ANGELO:\n",
            "Yea,, what I beseech you?\n",
            "\n",
            "ISABELLA:\n",
            "Look, and do beseech you, and do beseech you,\n",
            "I cannot do't.\n",
            "\n",
            "ANGELO:\n",
            "Pleased you to your soul,\n",
            "Were equal poise of sin and charity.\n",
            "\n",
            "ISABELLA:\n",
            "That I do beg his life, if it be sin,\n",
            "Heaven let me bear it! you granting of my suit,\n",
            "If that be sin, I'll make it my morn prayer\n",
            "To have it added to the faults of mine,\n",
            "And nothing of your answer.\n",
            "\n",
            "ANGELO:\n",
            "Nay, but hear me.\n",
            "Your sense pursues not mine: either you are ignorant,\n",
            "Or seem so craftily; and that's not good.\n",
            "\n",
            "ISABELLA:\n",
            "Let me be ignorant, and in nothing good,\n",
            "But graciously to know I am no better.\n",
            "\n",
            "ANGELO:\n",
            "Thus wisdom wishes to appear most bright\n",
            "When it doth tax itself; as these black masks\n",
            "Proclaim an enshield beauty ten times louder\n",
            "Than beauty could, display'd. But mark me;\n",
            "To be received plain, I'll speak more gross:\n",
            "Your brother is to die.\n",
            "\n",
            "ISABELLA:\n",
            "So.\n",
            "\n",
            "ANGELO:\n",
            "And his offence is so, as it appears,\n",
            "Accountant to the law upon that pain.\n",
            "\n",
            "ISABELLA:\n",
            "True.\n",
            "\n",
            "ANGELO:\n",
            "Admit no other way to save his life,--\n",
            "As I subscribe not that, nor any other,\n",
            "But in the loss of question,--that you, his sister,\n",
            "Finding yourself desired of such a person,\n",
            "Whose credit with the judge, or own great place,\n",
            "Could fetch your brother from the manacles\n",
            "Of the all-building law; and that there were\n",
            "No earthly mean to save him, but that either\n",
            "You must lay down the treasures of your body\n",
            "To this supposed, or else to let him suffer;\n",
            "What would you do?\n",
            "\n",
            "ISABELLA:\n",
            "As much\n",
            "---------------\n",
            "\n",
            "JULIET:\n",
            "Romeo, come to him, he is gone.\n",
            "\n",
            "Nurse:\n",
            "Romeo, he's dead, he's dead;\n",
            "The heaven cannot cannot: the day, Romeo banished.\n",
            "\n",
            "JULIET:\n",
            "O serpent heart, hid with a flowering face!\n",
            "Did ever dragon keep so fair a cave?\n",
            "Beautiful tyrant! fiend angelical!\n",
            "Dove-feather'd raven! wolvish-ravening lamb!\n",
            "Despised substance of divinest show!\n",
            "Just opposite to what thou justly seem'st,\n",
            "A damned saint, an honourable villain!\n",
            "O nature, what hadst thou to do in hell,\n",
            "When thou didst bower the spirit of a fiend\n",
            "In moral paradise of such sweet flesh?\n",
            "Was ever book containing such vile matter\n",
            "So fairly bound? O that deceit should dwell\n",
            "In such a gorgeous palace!\n",
            "\n",
            "Nurse:\n",
            "There's no trust,\n",
            "No faith, no honesty in men; all perjured,\n",
            "All forsworn, all dissemblers.\n",
            "Ah, where's my man? give me some aqua vitae:\n",
            "These griefs, these woes, these sorrows make me old.\n",
            "Shame come to Romeo!\n",
            "\n",
            "JULIET:\n",
            "Blister'd be thy tongue\n",
            "For such a wish! he was not born to shame:\n",
            "Upon his brow shame is ashamed to sit;\n",
            "For 'tis a throne where honour may be crown'd\n",
            "Sole monarch of the universal earth.\n",
            "O, what a beast was I to chide at him!\n",
            "\n",
            "Nurse:\n",
            "Will you speak well of him that kill'd your cousin?\n",
            "\n",
            "JULIET:\n",
            "Shall I speak ill of him that is my husband?\n",
            "Ah, poor my lord, what tongue shall smooth thy name,\n",
            "When I, thy three-hours wife, have mangled it?\n",
            "But, wherefore, villain, didst thou kill my cousin would have kill'd my cousin?\n",
            "That villain cousin would have kill'd my husband:\n",
            "Back, foolish tears, back to your native spring;\n",
            "Your tributary drops belong to woe,\n",
            "Which you, mistaking, offer up to joy.\n",
            "My husband lives, that Tybalt would have slain;\n",
            "And Tybalt's dead, that would have slain my husband:\n",
            "All this is comfort; wherefore weep I then?\n",
            "Some word there was, worser than Tybalt's death,\n",
            "That\n",
            "---------------\n",
            "\n",
            "ISABELLA:\n",
            "So she will Claudio, for his head; for death,\n",
            "And must die.\n",
            "\n",
            "CLAUDIO:\n",
            "Yes. Has he affections in him,\n",
            "That thus can make him bite the law by the law by the law by the law,\n",
            "Or of the law by the law by the law\n",
            "To implore it, that which in hell,\n",
            "For it, that were, that will not.\n",
            "\n",
            "ISABELLA:\n",
            "Yes, alas!\n",
            "\n",
            "CLAUDIO:\n",
            "Unhappily,\n",
            "And the new made them been as priests nor farther privileges?\n",
            "And the poor beetle,\n",
            "Whether the poor beetle,\n",
            "And the poor beetle, that we tread upon,\n",
            "In corporal sufferance finds a pang as great\n",
            "As when a giant dies.\n",
            "\n",
            "ISABELLA:\n",
            "Why give you me this shame?\n",
            "Think you I can a resolution fetch\n",
            "From flowery tenderness? If I must die,\n",
            "I will encounter darkness as a bride,\n",
            "And hug it in mine arms.\n",
            "\n",
            "ISABELLA:\n",
            "There spake my brother; there my father's grave\n",
            "Did utter forth a voice. Yes, thou must die:\n",
            "Thou art too noble to conserve a life\n",
            "In base appliances. This outward-sainted deputy,\n",
            "Whose settled visage and deliberate word\n",
            "Nips youth i' the head and follies doth emmew\n",
            "As falcon doth the fowl, is yet a devil\n",
            "His filth within being cast, he would appear\n",
            "A pond as hell.\n",
            "\n",
            "CLAUDIO:\n",
            "The prenzie Angelo!\n",
            "\n",
            "ISABELLA:\n",
            "O, 'tis the cunning livery of hell,\n",
            "The damned'st body to invest and cover\n",
            "In prenzie guards! Dost thou think, Claudio?\n",
            "If I would yield him my virginity,\n",
            "Thou mightst be freed.\n",
            "\n",
            "CLAUDIO:\n",
            "O heavens! it cannot be.\n",
            "\n",
            "ISABELLA:\n",
            "Yes, he would give't thee, from this rank offence,\n",
            "So to offend him still. This night's the time\n",
            "That I should do what I abhor to name,\n",
            "Or else thou diest to-morrow.\n",
            "\n",
            "CLAUDIO:\n",
            "Thou shalt not do't.\n",
            "\n",
            "ISABELLA:\n",
            "O, were it but my life,\n",
            "I'ld throw it down for your deliverance\n",
            "A\n",
            "---------------\n",
            "\n",
            "\n",
            "COMINIUS:\n",
            "I know not where you are not well:\n",
            "You have made fair consul, no further harm\n",
            "When corn at noon: forbad all names;\n",
            "He was a kind of nothing, titleless,\n",
            "Till he had forged himself a name o' the fire\n",
            "Of burning Rome.\n",
            "\n",
            "MENENIUS:\n",
            "Why, so: you have made good work!\n",
            "A pair of tribunes that have rack'd for Rome,\n",
            "To make coals cheap,--a noble memory!\n",
            "\n",
            "COMINIUS:\n",
            "I minded him how royal 'twas to pardon\n",
            "When it was less expected: he replied,\n",
            "It was a bare petition of a state\n",
            "To one whom they had punish'd.\n",
            "\n",
            "MENENIUS:\n",
            "Very well:\n",
            "Could he say less?\n",
            "\n",
            "COMINIUS:\n",
            "I offer'd to awaken his regard\n",
            "For's private friends: his answer to me was,\n",
            "He could not stay to pick them in a pile\n",
            "Of noisome musty chaff: he said 'twas folly,\n",
            "For one poor grain or two, to leave unburnt,\n",
            "And still to nose the offence.\n",
            "\n",
            "MENENIUS:\n",
            "For one poor grain or two!\n",
            "I am one of those; his mother, his child, his child,\n",
            "And this brave fellow too, we are the grains:\n",
            "You are the musty chaff; and you are smelt\n",
            "Above the moon: we must be burnt for you.\n",
            "SICINIUS:\n",
            "Nay, pray, be patient: if you refuse your aid\n",
            "In this so never-needed help, yet do not\n",
            "Upbraid's with our distress. But, sure, if you\n",
            "Would be your country's pleader, your good tongue,\n",
            "More than the instant army we can make,\n",
            "Might stop our countryman.\n",
            "\n",
            "MENENIUS:\n",
            "No, I'll not meddle.\n",
            "\n",
            "SICINIUS:\n",
            "Pray you, go to him.\n",
            "\n",
            "MENENIUS:\n",
            "What should I do?\n",
            "\n",
            "BRUTUS:\n",
            "Only make trial what your love can do\n",
            "For Rome, towards Marcius.\n",
            "\n",
            "MENENIUS:\n",
            "Well, and say that Marcius\n",
            "Return me, as Cominius is return'd,\n",
            "Unheard; what then?\n",
            "But as a discontented friend, grief-shot\n",
            "With his unkindness? say't be so?\n",
            "\n",
            "SICINIUS:\n",
            "---------------\n",
            "\n",
            "\n",
            "JULIET:\n",
            "My only love sprung from my only hate!\n",
            "Too rude, to one that is to my ghostly father.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "What less, good son's my ghostly father?\n",
            "\n",
            "I have forgot that name, and that name's woe.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "That's death: but where hast thou been, then?\n",
            "I'll tell thee, ere thou ask it me again.\n",
            "\n",
            "ROMEO:\n",
            "I have been feasting with mine enemy,\n",
            "Where on a sudden one hath wounded me,\n",
            "That's by me wounded me wounded: both our remedies\n",
            "Within thy help and holy physic lies:\n",
            "I bear no hatred, blessed man, for, lo,\n",
            "My intercession likewise steads my foe.\n",
            "FRIAR LAURENCE:\n",
            "Be plain, good son, and homely in thy drift;\n",
            "Riddling confession finds but riddling shrift.\n",
            "\n",
            "ROMEO:\n",
            "Then plainly know my heart's dear love is set\n",
            "On the fair daughter of rich Capulet:\n",
            "As mine on hers is set on mine;\n",
            "And all combined, save what thou must combine\n",
            "By holy marriage: when and how\n",
            "We met, we woo'd and made exchange of vow,\n",
            "I'll tell thee as we pass; but this I pray,\n",
            "That thou consent to marry us to-day.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Holy Saint Francis, what a change is here!\n",
            "Is Rosaline, whom thou didst love so dear,\n",
            "So soon forsaken? young men's love then lies\n",
            "Not truly in their hearts, but in their eyes.\n",
            "Jesu Maria, what a deal of brine\n",
            "Hath wash'd thy sallow cheeks for Rosaline!\n",
            "How much salt water thrown away in waste,\n",
            "To season love, that of it doth not yet thy sighs from heaven clears,\n",
            "Thy old groans ring yet in my ancient ears;\n",
            "Lo, here upon thy cheek the stain doth sit\n",
            "Of an old tear that is not wash'd off yet:\n",
            "If e'er thou wast thyself and these woes were all for Rosaline:\n",
            "Thou and these woes were all for Rosaline:\n",
            "And art thou changed? pronounce this sentence then,\n",
            "Women may fall, when there's no strength in men.\n",
            "\n",
            "ROMEO:\n",
            "Thou chid'st me oft for loving Rosaline.\n",
            "\n",
            "FRI\n",
            "---------------\n",
            "\n",
            "If you have no more benefit which you would you might well be\n",
            "As little skill to be the skill,\n",
            "Your precious image is so--to come; which I would wish you up:\n",
            "I'll think of my greetings that a brother,\n",
            "Which to a due by, but is so--\n",
            "\n",
            "PAULINA:\n",
            "Sir,\n",
            "I'll speak but dream'd of it.\n",
            "And I meant well,\n",
            "You should leave the sight of my brother,\n",
            "Andnd I meant love. Then all the queen,\n",
            "If you bid me to your children:\n",
            "The statue of the queen of her\n",
            "If she lived peerless, I do well-out\n",
            "Excels whatever yet you look'd upon\n",
            "Or hand of man hath done; therefore I keep it\n",
            "Lonely, apart. But here it is: prepare\n",
            "To see the life as lively mock'd as ever\n",
            "Still sleep mock'd death: behold, and say 'tis well.\n",
            "I like your silence, it the more shows off\n",
            "Your wonder: but yet speak; first, you, my liege,\n",
            "Comes it not something near?\n",
            "\n",
            "LEONTES:\n",
            "Her natural posture!\n",
            "Chide me, that I may say indeed\n",
            "Thou art Hermione; or rather, thou art she\n",
            "In thy not chiding, for she was as tender\n",
            "As infancy and grace. But yet, Paulina,\n",
            "Hermione was not so much wrinkled, nothing\n",
            "So aged as this seems.\n",
            "\n",
            "POLIXENES:\n",
            "O, not by much.\n",
            "\n",
            "PAULINA:\n",
            "So much the more our carver's excellence;\n",
            "Which lets go by some sixteen years and makes her\n",
            "As she lived now.\n",
            "\n",
            "LEONTES:\n",
            "As now she might have done,\n",
            "So much to my good comfort, as it is\n",
            "Now piercing to my soul. O, thus she stood,\n",
            "Even with such life of majesty, warm life,\n",
            "As now it coldly stands, when first I woo'd her!\n",
            "I am ashamed: does not the stone rebuke me\n",
            "For being more stone than it? O royal piece,\n",
            "There's magic in thy majesty, which has\n",
            "My evils conjured to remembrance and\n",
            "From thy admiring daughter took the spirits,\n",
            "Standing like stone with thee.\n",
            "\n",
            "PERDITA:\n",
            "And do not say\n",
            "---------------\n",
            "\n",
            "The slave, and am I did.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Art thou so hasty? I have stay'd for thee, pain and agony.\n",
            "And came I not at last to make the long-hearted countrymen,\n",
            "And came I not at Oxford here in their hands;\n",
            "And but now I not upon my head;\n",
            "And swell so much more,\n",
            "Than in England.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "I am I leave not stay who see him so much more:\n",
            "Than Bolingbroke have done.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Ah, who am I not but still hear,\n",
            "God for his mercy!\n",
            "And will the rest, the rest, they are gone!\n",
            "\n",
            "DUKE OF YORK:\n",
            "Here comes here! who are under this!\n",
            "\n",
            "QUEEN:\n",
            "Bring me my breast, I would to this world,\n",
            "And rise, if we may shake the sanctuary.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Welcome, my lord, what cause?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Give me the king, and beat your grace\n",
            "And hearten those that fight in your defence:\n",
            "Unsheathe your sword, good father; cry 'Saint George!'\n",
            "\n",
            "Edward, perjured Henry!\n",
            "Edward, perjured Henry!\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Plantagenet doth quit Plantagenet.\n",
            "EXETER:\n",
            "And wilt thou kneel for grace,\n",
            "And set thy minions, proud insulting boy!\n",
            "Becomes it thee to be thus bold in terms\n",
            "Before thy sovereign and thy lawful king?\n",
            "\n",
            "EDWARD:\n",
            "I am his king, and he should bow his knee;\n",
            "I was adopted heir by his consent:\n",
            "Since when, his oath is broke; for, as I hear,\n",
            "You, that are king, though he do wear the crown,\n",
            "Have caused him, by new act of parliament,\n",
            "To blot out me, and put his own son in.\n",
            "\n",
            "CLIFFORD:\n",
            "And reason too:\n",
            "Who should succeed the father but the son?\n",
            "\n",
            "RICHARD:\n",
            "Are you there, butcher? O, I cannot speak!\n",
            "\n",
            "CLIFFORD:\n",
            "Ay, crook-back, here I stand to answer thee,\n",
            "Or any he the proudest of thy sort.\n",
            "\n",
            "\n",
            "---------------\n",
            "\n",
            "I speak not, that which is made, nor is in her.\n",
            "\n",
            "ANGELO:\n",
            "He shall not be content to die tomorrow.\n",
            "\n",
            "ISABELLA:\n",
            "When, that which I do think, that might be,\n",
            "Lives not a heart-morrow. O, that a brother,\n",
            "It is the sliding of your bosom,\n",
            "And sliding of your bosom;\n",
            "Or a good lord; it oft falls out a merriment.\n",
            "\n",
            "ANGELO:\n",
            "Too late.\n",
            "\n",
            "ISABELLA:\n",
            "Ay; it not a strange, we speak not a vice,\n",
            "To have what we would have, we speak not what we mean:\n",
            "I something do excuse the thing I hate,\n",
            "For his advantage that I dearly love.\n",
            "\n",
            "ANGELO:\n",
            "We are all frail.\n",
            "\n",
            "ISABELLA:\n",
            "Else let my brother die,\n",
            "If not a feodary, but only he\n",
            "Owe and succeed thy weakness.\n",
            "\n",
            "ANGELO:\n",
            "Nay, women are frail too.\n",
            "\n",
            "ISABELLA:\n",
            "Ay, as the glasses where they view themselves;\n",
            "Which are as easy broke as they make forms.\n",
            "Women! Help Heaven! men their creation mar\n",
            "In profiting by them. Nay, call us ten times frail;\n",
            "For we are soft as our complexions are,\n",
            "And credulous to false prints.\n",
            "\n",
            "ANGELO:\n",
            "I think it well:\n",
            "And from this testimony of your own sex,--\n",
            "Since I suppose we are made to be no stronger\n",
            "Than faults may shake our frames,--let me be bold;\n",
            "I do arrest your words. Be that you are,\n",
            "That is, a woman; if you be more, you're none;\n",
            "If you are well express'd\n",
            "By all external warrants, show it now,\n",
            "By putting on the destined livery.\n",
            "\n",
            "ISABELLA:\n",
            "I have no tongue but one: gentle my lord,\n",
            "Let me entreat you speak the former language.\n",
            "\n",
            "ANGELO:\n",
            "Plainly conceive, I love you.\n",
            "\n",
            "ISABELLA:\n",
            "My brother did love Juliet,\n",
            "And you tell me that he shall die for it.\n",
            "\n",
            "ANGELO:\n",
            "He shall not, Isabel, if you give me love.\n",
            "\n",
            "ISABELLA:\n",
            "I know your virtue hath a licence in't\n",
            "---------------\n",
            "\n",
            "LEONTES:\n",
            "What! look, that thy wife?\n",
            "\n",
            "CAMILLO:\n",
            "He has betray'd with his anchor;\n",
            "Account me to the memory of his ground,\n",
            "And mar the farthest off,\n",
            "I am his cupbearer,--for 't and my entreaty.\n",
            "\n",
            "LEONTES:\n",
            "I am question you will questionHow, Camillo,\n",
            "And, so, so your queen's from your precious mistress,\n",
            "That I see she would call you of yet you adventure\n",
            "So your queen's be made.\n",
            "\n",
            "PAULINA:\n",
            "Sir,\n",
            "I am question'd you kiss her,\n",
            "Had I mine own accord I'll off;\n",
            "But first I'll do my errand. The good queen,\n",
            "For she is good, hath brought you forth a daughter;\n",
            "Here 'tis; commends it to your blessing.\n",
            "\n",
            "LEONTES:\n",
            "Out!\n",
            "A mankind witch! Hence with her, out o' door:\n",
            "A most intelligencing bawd!\n",
            "\n",
            "PAULINA:\n",
            "Not so:\n",
            "I am as ignorant in that as you\n",
            "In so entitling me, I'll warrant,\n",
            "As this world goes, to pass for honest.\n",
            "\n",
            "LEONTES:\n",
            "Traitors!\n",
            "Will you not push her out? Give her the bastard.\n",
            "Thou dotard! thou art woman-tired, unroosted\n",
            "By thy dame Partlet here. Take up the bastard;\n",
            "Take't up, I say; give't to thy crone.\n",
            "\n",
            "PAULINA:\n",
            "For ever\n",
            "Unvenerable be thy hands, if thou\n",
            "Takest up the princess by that forced baseness\n",
            "Which he has put upon't!\n",
            "\n",
            "LEONTES:\n",
            "He dreads his wife.\n",
            "\n",
            "PAULINA:\n",
            "So I would you did; then 'twere past all doubt\n",
            "You'ld call your children yours.\n",
            "\n",
            "LEONTES:\n",
            "A nest of traitors!\n",
            "\n",
            "ANTIGONUS:\n",
            "I am none, by this good light.\n",
            "\n",
            "PAULINA:\n",
            "Nor I, nor any\n",
            "But one that's here, for he\n",
            "The sacred honour of himself, his queen's,\n",
            "His hopeful son's, his babe's, betrays to slander,\n",
            "Whose sting is sharper than the sword's;\n",
            "and will not--\n",
            "For, as the case now stands, it is a curse\n",
            "He cannot be compell'd to\n",
            "---------------\n",
            "\n",
            "Not to be so much more accusation\n",
            "Than history of their familiarity,\n",
            "Cs ever touch'd conjecture, nought for approbation\n",
            "That lack'd sight only seeing, nought for approbation\n",
            "But only seeing, all other circumstances\n",
            "Made up to the deed, doth push on this proceeding:\n",
            "Yet, for a greater confirmation,\n",
            "For in an act of this importance 'twere\n",
            "Most piteous to be wild, I have dispatch'd in post\n",
            "To sacred Delphos, to Apollo's temple,\n",
            "Cleomenes and Dion, whom you know\n",
            "Of stuff'd sufficiency: now from the oracle\n",
            "They will bring all; whose spiritual counsel had,\n",
            "Shall stop or spur me. Have I done well?\n",
            "\n",
            "First Lord:\n",
            "Well done, my lord.\n",
            "\n",
            "LEONTES:\n",
            "Though I am satisfied and need no more\n",
            "Than what I know, yet shall the oracle\n",
            "Give rest to the minds of others, such as he\n",
            "Whose ignorant credulity will not\n",
            "Come up to the truth. So have we thought it good\n",
            "From our free person she should be confined,\n",
            "Lest that the treachery of the two fled hence\n",
            "Be left her to perform. Come, follow us;\n",
            "We are to speak in public; for this business\n",
            "Will raise us all.\n",
            "\n",
            "ANTIGONUS:\n",
            "\n",
            "PAULINA:\n",
            "The keeper of the prison, call to him;\n",
            "let him have knowledge who I am.\n",
            "Good lady,\n",
            "No court in Europe is too good for thee;\n",
            "What dost thou then in prison?\n",
            "Now, good sir,\n",
            "You know me, do you not?\n",
            "\n",
            "Gaoler:\n",
            "For a worthy lady\n",
            "And one whom much I honour.\n",
            "\n",
            "PAULINA:\n",
            "Pray you then,\n",
            "Conduct me to the queen.\n",
            "\n",
            "Gaoler:\n",
            "I may not, madam:\n",
            "To the contrary I have express commandment.\n",
            "\n",
            "PAULINA:\n",
            "Here's ado,\n",
            "To lock up honesty and honour from\n",
            "The access of gentle visitors!\n",
            "Is't lawful, pray you,\n",
            "To see her women? any of them? Emilia?\n",
            "\n",
            "Gaoler:\n",
            "So please you, madam,\n",
            "To put apart these your attendants, I\n",
            "Shall bring Emilia forth.\n",
            "\n",
            "PAULINA:\n",
            "I pray now, call her.\n",
            "Withdraw yourselves.\n",
            "\n",
            "G\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}